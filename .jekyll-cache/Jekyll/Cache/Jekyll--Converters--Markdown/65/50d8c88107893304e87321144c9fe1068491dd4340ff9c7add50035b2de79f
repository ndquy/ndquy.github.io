I"†_<p>BÃ i trÆ°á»›c mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u má»i ngÆ°á»i cÃ¡ch Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y, trong Ä‘Ã³ má»¥c Ä‘Ã­ch cá»§a viá»‡c huáº¥n luyá»‡n lÃ  Ä‘á»ƒ tÃ¬m ra cÃ¡c tham sá»‘ mÃ  táº¡i Ä‘Ã³ hÃ m chi phÃ­ (hÃ m máº¥t mÃ¡t) Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t.</p>

<p>Trong toÃ¡n tá»‘i Æ°u, viá»‡c tÃ¬m ra cá»±c trá»‹ cá»§a hÃ m sá»‘ ráº¥t phá»• biáº¿n. CÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ tÃ¬m cá»±c trá»‹ hÃ m sá»‘, trong Ä‘Ã³ cÃ¡ch phá»• biáº¿n nháº¥t lÃ  tÃ¬m Ä‘áº¡o hÃ m rá»“i giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m báº±ng 0, cÃ¡c nghiá»‡m sáº½ lÃ  há»¯u háº¡n vÃ  thay tá»«ng nghiá»‡m vÃ o hÃ m sá»‘ ta sáº½ Ä‘Æ°á»£c cÃ¡c giÃ¡ trá»‹ cá»±c tiá»ƒu, sau Ä‘Ã³ láº¥y nghiá»‡m lÃ m cho hÃ m sá»‘ cÃ³ giÃ¡ trá»‹ nhá» nháº¥t.</p>

<p>Tuy nhiÃªn thÃ¬ khÃ´ng pháº£i lÃºc nÃ o chÃºng ta cÅ©ng cÃ³ thá»ƒ tÃ¬m ra Ä‘Æ°á»£c Ä‘áº¡o hÃ m cÅ©ng nhÆ° giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m. LÃ½ do lÃ  do hÃ m sá»‘ cÃ³ Ä‘áº¡o hÃ m phá»©c táº¡p, dá»¯ liá»‡u cÃ³ nhiá»u chiá»uâ€¦ VÃ¬ váº­y ngÆ°á»i ta nghÄ© ra 1 cÃ¡ch Ä‘á»ƒ tÃ¬m cá»±c trá»‹ (cá»±c tiá»ƒu/cá»±c Ä‘áº¡i) báº±ng phÆ°Æ¡ng phÃ¡p Gradient Descent.</p>

<h1 id="gradient-descent-lÃ -gÃ¬">Gradient Descent lÃ  gÃ¬?</h1>

<p>Gradient Descent (GD) lÃ  thuáº­t toÃ¡n tÃ¬m tá»‘i Æ°u chung cho cÃ¡c hÃ m sá»‘. Ã tÆ°á»Ÿng chung cá»§a GD lÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘á»ƒ láº·p Ä‘i láº·p láº¡i thÃ´ng qua má»—i dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ giáº£m thiá»ƒu hÃ m chi phÃ­.</p>

<p>Giáº£ sá»­ báº¡n bá»‹ láº¡c trÃªn 1 ngá»n nÃºi vÃ  trong sÆ°Æ¡ng mÃ¹ dÃ y Ä‘áº·c, báº¡n chá»‰ cÃ³ thá»ƒ cáº£m tháº¥y Ä‘á»™ dá»‘c cá»§a máº·t Ä‘áº¥t dÆ°á»›i chÃ¢n báº¡n. Má»™t cÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ nhanh chÃ³ng xuá»‘ng chÃ¢n nÃºi lÃ  xuá»‘ng dá»‘c theo hÆ°á»›ng dá»‘c nháº¥t. ÄÃ¢y chÃ­nh lÃ  Ã½ tÆ°á»Ÿng cá»§a Gradient Descent thá»±c hiá»‡n, táº¡i má»—i Ä‘iá»ƒm cá»§a hÃ m sá»‘, nÃ³ sáº½ xÃ¡c Ä‘á»‹nh Ä‘á»™ dá»‘c sau Ä‘Ã³ Ä‘i ngÆ°á»£c láº¡i vá»›i hÆ°á»›ng cá»§a Ä‘á»™ dá»‘c Ä‘áº¿n khi nÃ o Ä‘á»™ dá»‘c táº¡i chá»— Ä‘Ã³ báº±ng 0 (cá»±c tiá»ƒu)</p>

<p><img src="/assets/img/blog/understanding-gradient-descent.png" alt="MÃ´ phá»ng thuáº­t toÃ¡n Gradient Descent" />
<em>MÃ´ phá»ng thuáº­t toÃ¡n Gradient Descent</em></p>

<p>Gradient Descent lÃ  má»™t thuáº­t toÃ¡n tá»‘i Æ°u láº·p (iterative optimization algorithm) Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n Machine Learning vÃ  Deep Learning (thÆ°á»ng lÃ  cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u lá»“i â€” Convex Optimization) vá»›i má»¥c tiÃªu lÃ  tÃ¬m má»™t táº­p cÃ¡c biáº¿n ná»™i táº¡i (internal parameters) cho viá»‡c tá»‘i Æ°u models. Trong Ä‘Ã³:</p>

<ul>
  <li>Gradient: lÃ  tá»· lá»‡ Ä‘á»™ nghiÃªng cá»§a Ä‘Æ°á»ng dá»‘c (rate of inclination or declination of a slope). Vá» máº·t toÃ¡n há»c, Gradient cá»§a má»™t hÃ m sá»‘ lÃ  Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ Ä‘Ã³ tÆ°Æ¡ng á»©ng vá»›i má»—i biáº¿n cá»§a hÃ m. Äá»‘i vá»›i hÃ m sá»‘ Ä‘Æ¡n biáº¿n, chÃºng ta sá»­ dá»¥ng khÃ¡i niá»‡m Derivative thay cho Gradient.</li>
  <li>Descent: lÃ  tá»« viáº¿t táº¯t cá»§a descending, nghÄ©a lÃ  giáº£m dáº§n.</li>
</ul>

<p>Gradient Descent cÃ³ nhiá»u dáº¡ng khÃ¡c nhau nhÆ° Stochastic Gradient Descent (SGD), Mini-batch SDG. NhÆ°ng vá» cÆ¡ báº£n thÃ¬ Ä‘á»u Ä‘Æ°á»£c thá»±c thi nhÆ° sau:</p>

<ul>
  <li>Khá»Ÿi táº¡o biáº¿n ná»™i táº¡i.</li>
  <li>ÄÃ¡nh giÃ¡ model dá»±a vÃ o biáº¿n ná»™i táº¡i vÃ  hÃ m máº¥t mÃ¡t (Loss function).</li>
  <li>Cáº­p nháº­t cÃ¡c biáº¿n ná»™i táº¡i theo hÆ°á»›ng tá»‘i Æ°u hÃ m máº¥t mÃ¡t (finding optimal points).</li>
  <li>Láº·p láº¡i bÆ°á»›c 2, 3 cho tá»›i khi thá»a Ä‘iá»u kiá»‡n dá»«ng.</li>
  <li>CÃ´ng thá»©c cáº­p nháº­t cho GD cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t lÃ :</li>
</ul>

<p>[\theta^{(next step)} =\theta - \eta  âˆ‡_\theta]</p>

<p>trong Ä‘Ã³ $Î¸$ lÃ  táº­p cÃ¡c biáº¿n cáº§n cáº­p nháº­t, $Î·$ lÃ  tá»‘c Ä‘á»™ há»c (learning rate), $â–½Ó¨f(Î¸)$ lÃ  Gradient cá»§a hÃ m máº¥t mÃ¡t f theo táº­p Î¸.</p>

<h1 id="learning-rate">Learning rate</h1>

<p>CÃ³ 1 tham sá»‘ quan trá»ng trong Gradient Descent Ä‘Ã³ lÃ  giÃ¡ trá»‹ Ä‘á»™ lá»›n cá»§a má»—i láº§n di chuyá»ƒn (giá»‘ng nhÆ° Ä‘á»™ dÃ i sáº£i chÃ¢n khi báº¡n leo xuá»‘ng dá»‘c).</p>

<p>Tham sá»‘ nÃ y Ä‘Æ°á»£c gá»i lÃ  learning rate (tá»‘c Ä‘á»™ há»c). Náº¿u learning rate quÃ¡ nhá», thuáº­t toÃ¡n sáº½ pháº£i thá»±c hiá»‡n nhiá»u bÆ°á»›c Ä‘á»ƒ há»™i tá»¥ vÃ  sáº½ máº¥t nhiá»u thá»i gian.</p>

<p>Tuy nhiÃªn náº¿u learning rate quÃ¡ lá»›n sáº½ khiáº¿n thuáº­t toÃ¡n Ä‘i qua cá»±c tiá»ƒu, vÃ  vÆ°á»£t háº³n ra ngoÃ i khiáº¿n thuáº­t toÃ¡n khÃ´ng thá»ƒ há»™i tá»¥ Ä‘Æ°á»£c.</p>

<p><img src="/assets/img/blog/1_ShhdswkZTInut3L6Nbbw3Q.png" alt="Sá»± áº£nh hÆ°á»Ÿng cá»§a learning rate Ä‘áº¿n mÃ´ hÃ¬nh" />
<em>Sá»± áº£nh hÆ°á»Ÿng cá»§a learning rate Ä‘áº¿n mÃ´ hÃ¬nh</em></p>

<p>Trong thá»±c táº¿, khÃ´ng pháº£i hÃ m sá»‘ nÃ o cÅ©ng chá»‰ cÃ³ 1 cá»±c tiá»ƒu. Ta sáº½ cÃ³ khÃ¡i niá»‡m cá»±c tiá»ƒu cá»¥c bá»™ vÃ  cá»±c tiá»ƒu toÃ n cá»¥c. Hiá»ƒu nÃ´m na nÃ³ giá»‘ng nhÆ° cÃ¡c há»‘ hoáº·c cÃ¡c táº£ng Ä‘Ã¡ á»Ÿ trÃªn nÃºi khi báº¡n Ä‘ang leo xuá»‘ng nÃºi. LÃºc nÃ y viá»‡c tÃ¬m ra cá»±c tiá»ƒu sáº½ trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n. Xem hÃ¬nh sau Ä‘á»ƒ biáº¿t chi tiáº¿t:</p>

<p><img src="/assets/img/blog/1_QXYOKUUQMsJrRnSXRCzHcA.png" alt="Cá»±c tiá»ƒu hÃ m sá»‘" />
<em>Cá»±c trá»‹ hÃ m sá»‘</em></p>

<p>Sáº½ cÃ³ 2 váº¥n Ä‘á» lÃºc nÃ y Ä‘á»‘i vá»›i GD:</p>

<p>Äiá»ƒm xuáº¥t phÃ¡t cÃ³ thá»ƒ á»Ÿ bÃªn trÃ¡i hoáº·c bÃªn pháº£i, náº¿u xuáº¥t phÃ¡t tá»« bÃªn trÃ¡i, thuáº­t toÃ¡n sáº½ há»™i tá»¥ á»Ÿ local minimum (cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng) mÃ  khÃ´ng Ä‘i Ä‘áº¿n Ä‘Æ°á»£c global minium (cá»±c tiá»ƒu toÃ n cá»¥c).</p>

<p>Hoáº·c náº¿u xuáº¥t phÃ¡t tá»« bÃªn pháº£i sáº½ pháº£i máº¥t nhiá»u thá»i gian Ä‘á»ƒ vÆ°á»£t qua Plateau Ä‘á»ƒ Ä‘áº¿n Ä‘Æ°á»£c global minimum vÃ  náº¿u káº¿t thÃºc thuáº­t toÃ¡n quÃ¡ sá»›m thÃ¬ sáº½ khÃ´ng Ä‘áº¿n Ä‘Æ°á»£c global minimum.</p>

<p>BÃ i trÆ°á»›c chÃºng ta cÃ³ sá»­ dá»¥ng hÃ m chi phÃ­ MSE cho bÃ i toÃ¡n há»“i quy tuyáº¿n tÃ­nh, ráº¥t may lÃ  hÃ m nÃ y lÃ  hÃ m lá»“i. NghÄ©a lÃ  náº¿u 1 Ä‘Æ°á»ng tháº³ng ná»‘i 2 Ä‘iá»ƒm báº¥t kÃ¬ trÃªn Ä‘á»“ thá»‹ hÃ m lá»“i thÃ¬ Ä‘Æ°á»ng tháº³ng nÃ y sáº½ khÃ´ng cáº¯t Ä‘á»“ thá»‹. Äiá»u nÃ y nghÄ©a lÃ  khÃ´ng cÃ³ cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng (local minimum) mÃ  chá»‰ cÃ³ 1 cá»±c tiá»ƒu toÃ n cá»¥c. ÄÃ¢y cÅ©ng lÃ  má»™t hÃ m liÃªn tá»¥c cÃ³ Ä‘á»™ dá»‘c khÃ´ng bao giá» thay Ä‘á»•i Ä‘á»™t ngá»™t. VÃ¬ váº­y á» Ä‘Ã¢y GD cÃ³ 1 váº¥n Ä‘á», Ä‘Ã³ lÃ  nÃ³ sáº½ khÃ´ng tiáº¿n gáº§n Ä‘áº¿n Ä‘Æ°á»£c global minimum (trá»« khi thá»i gian há»c Ä‘á»§ lÃ¢u vÃ  learning rate Ä‘á»§ nhá»)</p>

<p>TrÃªn thá»±c táº¿, hÃ m chi phÃ­ cÃ³ dáº¡ng Ä‘á»“ thá»‹ giá»‘ng chiáº¿c bÃ¡t, náº¿u cÃ¡c feature (Ä‘áº·c Ä‘iá»ƒm cá»§a Ä‘áº§u vÃ o -  thÃ nh pháº§n cá»§a vector X)  cÃ³ cÃ¹ng pháº¡m vi giÃ¡ trá»‹, thÃ¬ miá»‡ng bÃ¡t sáº½ trÃ²n vÃ  Ä‘á»ƒ GD Ä‘i xuá»‘ng Ä‘Ã¡y bÃ¡t sáº½ nhanh hÆ¡n. Náº¿u cÃ¡c feature khÃ¡c pháº¡m vi giÃ¡ trá»‹ thÃ¬ miá»‡ng bÃ¡t sáº½ bá»‹ kÃ©o dÃ i ra vÃ  viá»‡c Ä‘i xuá»‘ng Ä‘Ã¡y bÃ¡t sáº½ tá»‘n thá»i gian hÆ¡n. ÄÃ¢y lÃ  lÃ½ do vÃ¬ sao cÃ¡c feature cá»§a vector Ä‘áº§u vÃ o X cáº§n pháº£i Ä‘Æ°á»£c scaling  (cÄƒn chá»‰nh).</p>

<p><img src="/assets/img/blog/0_W6ERHnn2kU05FEYs.png" alt="GD cÃ³ scaling vÃ  khÃ´ng scaling" />
<em>GD cÃ³ scaling vÃ  khÃ´ng scaling</em></p>

<p>NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, á»Ÿ bÃªn pháº£i thuáº­t toÃ¡n Gradient Descent Ä‘i tháº³ng vá» Ä‘iá»ƒm tá»‘i thiá»ƒu, do Ä‘Ã³ nhanh chÃ³ng Ä‘áº¡t Ä‘Æ°á»£c cá»±c tiá»ƒu toÃ n cá»¥c, trong khi bÃªn trÃ¡i, nÃ³ Ä‘i theo hÆ°á»›ng gáº§n nhÆ° trá»±c giao vá»›i hÆ°á»›ng vá» cá»±c thiá»ƒu toÃ n cá»¥c, vÃ¬ váº­y nÃ³ káº¿t thÃºc báº±ng 1 hÃ nh trÃ¬nh dÃ i xuá»‘ng má»™t 1 máº·t gáº§n nhÆ° báº±ng pháº³ng. Cuá»‘i cÃ¹ng nÃ³ sáº½ Ä‘áº¡t Ä‘áº¿n má»©c cá»±c tiá»ƒu, nhÆ°ng sáº½ máº¥t nhiá»u thá»i gian.</p>

<blockquote>
  <p>Khi báº¡n thá»±c hiá»‡n thuáº­t toÃ¡n Gradient Descent, báº¡n nÃªn Ä‘Æ°a cÃ¡c feature vá» cÃ¹ng pháº¡m vi giÃ¡ trá»‹ (sá»­ dá»¥ng  StandardScaler cá»§a thÆ° viá»‡n Scikit-Learn</p>
</blockquote>

<h1 id="batch-gradient-descent">Batch Gradient Descent</h1>

<p>Äá»ƒ thá»±c hiá»‡n thuáº­t toÃ¡n Gradient Descent, chÃºng ta pháº£i tÃ¬m Ä‘Æ°á»£c Ä‘áº¡o hÃ m cá»§a hÃ m chi phÃ­ áº£nh hÆ°á»Ÿng Ä‘áº¿n tá»«ng tham sá»‘ cá»§a mÃ´ hÃ¬nh $ \theta_j $. NÃ³i khÃ¡c Ä‘i, cáº§n pháº£i xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c giÃ¡ trá»‹ hÃ m chi phÃ­ thay Ä‘á»•i tháº¿ nÃ o náº¿u thay Ä‘á»•i $ \theta_j $. CÃ¡i nÃ y Ä‘Æ°á»£c gá»i lÃ  Ä‘áº¡o hÃ m riÃªng (partial derivative).</p>

<p>Biá»ƒu thá»©c sau sáº½ dÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m chi phÃ­ cho tham sá»‘ $ \theta_j $, Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  $ \frac{\delta}{\delta\theta_j}MSE(\theta) $:</p>

<p>[\frac{\delta}{\delta\theta_j}MSE(\theta)  = \frac{2}{m}\sum_{i=1}^m(\theta^Tx^{(i)} - y^{(i)})x^{(i)}_j]</p>

<p>Thay vÃ¬ tÃ­nh tá»«ng Ä‘áº¡o hÃ m thÃ nh pháº§n, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ´ng thá»©c sau Ä‘á»ƒ tÃ­nh táº¥t cáº£ trong 1 bÆ°á»›c. Vector Ä‘á»™ dá»‘c, kÃ½ hiá»‡u $ âˆ‡_\theta MSE(\theta) $ lÃ  Ä‘áº¡o hÃ m riÃªng (vector Ä‘á»™ dá»‘c) cho cÃ¡c tham sá»‘ $ \theta) $ cá»§a mÃ´ hÃ¬nh.</p>

<p>Khi chÃºng ta cÃ³ vector Ä‘á»™c dá»‘c vÃ  vá»‹ trÃ­ hiá»‡n táº¡i, chÃºng ta chá»‰ cáº§n Ä‘i ngÆ°á»£c láº¡i vá»›i vector Ä‘á»™ dá»‘c. NghÄ©a lÃ  ta pháº£i trá»« Î¸ Ä‘i 1 giÃ¡ trá»‹ lÃ  $ âˆ‡_\theta MSE(\theta) $. LÃºc nÃ y ta sáº½ sá»­ dá»¥ng tham sá»‘ learning rate $ \eta $ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ cá»§a bÆ°á»›c xuá»‘ng dá»‘c báº±ng cÃ¡ch nhÃ¢n vÃ o.</p>

<p>[\theta^{(next step)} =\theta - \eta  âˆ‡_\theta MSE(\theta)]</p>

<p>BÃ¢y giá» chÃºng ta sáº½ thá»±c hiá»‡n thá»­ bÃ i trÆ°á»›c báº±ng Python:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> --><td class="rouge-code"><pre><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># learning rate n_iterations = 1000 m=100
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># random initialization
</span><span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
	<span class="n">gradients</span> <span class="o">=</span> <span class="mi">2</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">X_b</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradients</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Káº¿t quáº£ cá»§a theta sáº½ nhÆ° sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre><span class="o">&gt;&gt;&gt;</span> <span class="n">theta</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">4.21509616</span><span class="p">],</span>
<span class="p">[</span><span class="mf">2.77011339</span><span class="p">]])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>NhÆ° váº­y lÃ  cÃ³ váº» GD hoáº¡t Ä‘á»™ng tá»‘t. NhÆ°ng náº¿u sá»­ dá»¥ng learning rate khÃ¡c nhau thÃ¬ sao? HÃ¬nh sau sáº½ cho ta tháº¥y 10 bÆ°á»›c láº·p cá»§a thuáº­t toÃ¡n vá»›i cÃ¡c learning rate khÃ¡c nhau:</p>

<p><img src="/assets/img/blog/GD-test.png" alt="GD cÃ³ scaling vÃ  khÃ´ng scaling" />
<em>GD vá»›i cÃ¡c learning rate khÃ¡c nhau</em></p>

<p>á» bÃªn trÃ¡i, learning rate quÃ¡ tháº¥p: thuáº­t toÃ¡n cuá»‘i cÃ¹ng sáº½ há»™i tá»¥, nhÆ°ng sáº½ máº¥t nhiá»u thá»i gian. á» giá»¯a, learning rate cÃ³ váº» khÃ¡ tá»‘t: chá»‰ trong vÃ i láº§n láº·p thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥. á» bÃªn pháº£i, learning rate quÃ¡ cao: thuáº­t toÃ¡n phÃ¢n ká»³, nháº£y kháº¯p nÆ¡i vÃ  ngÃ y cÃ ng rá»i xa cá»±c tiá»ƒu á»Ÿ má»—i bÆ°á»›c láº·p.</p>

<p>á» bÃ i sau, mÃ¬nh sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡c Ä‘á»ƒ tÃ¬m learning rate phÃ¹ há»£p cho tá»«ng bÃ i toÃ¡n. CÃ²n 1 váº¥n Ä‘á» ná»¯a Ä‘Ã³ lÃ  sá»‘ láº§n láº·p cá»§a thuáº­t toÃ¡n, náº¿u thá»±c hiá»‡n Ã­t bÆ°á»›c láº·p, thuáº­t toÃ¡n cÃ³ thá»ƒ khÃ´ng Ä‘áº¿n Ä‘Æ°á»£c cá»±c tiá»ƒu, cÃ²n náº¿u nhiá»u quÃ¡ thuáº­t toÃ¡n cÃ³ thá»ƒ máº¥t nhiá»u thá»i gian thá»±c hiá»‡n mÃ  cÃ¡c tham sá»‘ háº§u nhÆ° khÃ´ng thay Ä‘á»•i do Ä‘Ã£ Ä‘áº¿n Ä‘Ã­ch. Giáº£i phÃ¡p Ä‘Æ¡n giáº£n Ä‘Ã³ lÃ  ta sáº½ dá»«ng thuáº­t toÃ¡n khi giÃ¡ trá»‹ chuáº©n cá»§a vector Ä‘á»™ dá»‘c (gradient vector) Ä‘á»§ nhá» vÃ  nhá» hÆ¡n 1 sá»‘ Îµ (Ä‘Æ°á»£c gá»i lÃ  dung sai) Ä‘á»ƒ cháº¯c cháº¯n thuáº­t toÃ¡n Ä‘Ã£ Ä‘áº¿n Ä‘iá»ƒm há»™i tá»¥.</p>

<blockquote>
  <p>Khi hÃ m chi phÃ­ lÃ  hÃ m lá»“i vÃ  Ä‘á»™ dá»‘c cá»§a nÃ³ khÃ´ng thay Ä‘á»•i Ä‘á»™t ngá»™t (nhÆ° trÆ°á»ng há»£p cá»§a hÃ m MSE), Batch Gradient Descent vá»›i learning rate cá»‘ Ä‘á»‹nh cuá»‘i cÃ¹ng sáº½ há»™i tá»¥ Ä‘áº¿n cá»±c tiá»ƒu, tuy nhiÃªn báº¡n cÃ³ thá»ƒ pháº£i chá» má»™t khoáº£ng thá»i gian lÃ¢u hÆ¡n: nÃ³ cÃ³ thá»ƒ máº¥t cÃ¡c bÆ°á»›c láº·p O(1 /Îµ) Ä‘á»ƒ cá»±c tiá»ƒu trong pháº¡m vi Îµ phá»¥ thuá»™c vÃ o hÃ¬nh dáº¡ng cá»§a hÃ m chi phÃ­. Náº¿u báº¡n chia Îµ cho 10 Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n thÃ¬ thuáº­t toÃ¡n cÃ³ thá»ƒ pháº£i cháº¡y lÃ¢u hÆ¡n khoáº£ng 10 láº§n.</p>
</blockquote>

<h1 id="stochastic-gradient-descent">Stochastic Gradient Descent</h1>

<p>Thay vÃ¬ sá»­ dá»¥ng toÃ n bá»™ táº­p huáº¥n luyá»‡n thÃ¬ Stochastics Gradient Descent (SGD) sáº½ láº¥y ngáº«u nhiÃªn 1 pháº§n tá»­ á»Ÿ táº­p huáº¥n luyá»‡n vÃ  thá»±c hiá»‡n tÃ­nh láº¡i vector Ä‘á»™ dá»‘c dá»±a chá»‰ dá»±a trÃªn 1 Ä‘iá»ƒm dá»¯ liá»‡u, sau Ä‘Ã³ láº·p Ä‘i láº·p láº¡i Ä‘áº¿n khi káº¿t thÃºc. VÃ  viá»‡c tÃ­nh toÃ¡n dá»±a trÃªn 1 Ä‘iá»ƒm dá»¯ liá»‡u sáº½ khiáº¿n thuáº­t toÃ¡n cháº¡y nhanh hÆ¡n bá»Ÿi cÃ³ ráº¥t Ã­t dá»¯ liá»‡u cáº§n xá»­ lÃ½ á»Ÿ má»—i vÃ²ng láº·p. VÃ  Ä‘iá»u nÃ y cÅ©ng giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i nhá»¯ng dá»¯ liá»‡u lá»›n vÃ¬ má»—i vÃ²ng láº·p chá»‰ cáº§n Ä‘Æ°a 1 Ä‘iá»ƒm dá»¯ liá»‡u vÃ o trong bá»™ nhá»›.</p>

<p>Máº·t khÃ¡c do tÃ­nh cháº¥t ngáº«u nhiÃªn cá»§a dá»¯ liá»‡u Ä‘Æ°a vÃ o nÃªn trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, thay vÃ¬ hÃ m chi phÃ­ giáº£m tá»« tá»« giá»‘ng Batch GD thÃ¬ hÃ m chi phÃ­ cá»§a SGD sáº½ lÃºc tÄƒng lÃºc giáº£m nhÆ°ng sáº½ giáº£m dáº§n theo khoáº£ng thá»i gian. Dáº§n dáº§n nghiá»‡m cá»§a bÃ i toÃ¡n nÃ³ sáº½ tiá»‡m cáº­n ráº¥t gáº§n vá»›i cá»±c tiá»ƒu nhÆ°ng khi Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c cá»±c tiá»ƒu thÃ¬ giÃ¡ trá»‹ hÃ m chi phÃ­ sáº½ liÃªn tá»¥c thay Ä‘á»•i mÃ  khÃ´ng giá»¯ á»•n Ä‘á»‹nh. Khi gáº·p Ä‘iá»u kiá»‡n dá»«ng ta sáº½ Ä‘Æ°á»£c bá»™ tham sá»‘ cuá»‘i cÃ¹ng Ä‘á»§ tá»‘t, nhÆ°ng chÆ°a tháº­t sá»± tá»‘i Æ°u.</p>

<p>Khi hÃ m chi phÃ­ liÃªn tá»¥c thay Ä‘á»•i cÃ³ thá»ƒ giÃºp thuáº­t toÃ¡n nháº£y ra khá»i cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng. VÃ¬ váº­y SGD cÃ³ cÆ¡ há»™i Ä‘á»ƒ tÃ¬m Ä‘Æ°á»£c cá»±c trá»‹ toÃ n cá»¥c hÆ¡n lÃ  Batch Gradient Descent. VÃ¬ váº­y lá»±a chá»n ngáº«u nhiÃªn dá»¯ liá»‡u sáº½ giÃºp thoÃ¡t khá»i nghiá»‡m tá»‘i Æ°u cá»¥c bá»™ nhÆ°ng Ä‘iá»u Ä‘Ã³ nghÄ©a lÃ  thuáº­t toÃ¡n cÅ©ng khÃ´ng bao giá» cÃ³ nghiá»‡m cá»±c tiá»ƒu.</p>

<h2 id="giáº£m-dáº§n-learning-rate">Giáº£m dáº§n learning rate</h2>

<p>NgÆ°á»i ta nghÄ© ra thÃªm Ä‘Æ°á»£c 1 cÃ¡ch giáº£i quyáº¿t Ä‘Ã³ lÃ  giáº£m dáº§n learning rate trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. CÃ¡c vÃ²ng láº·p Ä‘áº§u tiÃªn chÃºng ta sáº½ Ä‘á»ƒ learning rate lá»›n Ä‘á»ƒ thoÃ¡t khá»i cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng, sau Ä‘Ã³ giáº£m dáº§n tá»‘c Ä‘á»™ há»c Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c cá»±c tiá»ƒu toÃ n cá»¥c. HÃ m Ä‘á»ƒ xÃ¡c Ä‘á»‹nh learning rate á»Ÿ má»—i láº§n láº·p Ä‘Æ°á»£c gá»i lÃ  hÃ m lÃªn lá»‹ch cho tá»‘c Ä‘á»™ há»c.</p>

<p>Náº¿u learning rate giáº£m quÃ¡ nhanh, thuáº­t toÃ¡n cÃ³ thá»ƒ bá»‹ dá»«ng á»Ÿ Ä‘iá»ƒm cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng, hoáº·c káº¿t thÃºc á»Ÿ lÆ°ng chá»«ng khi chÆ°a Ä‘áº¿n Ä‘iá»ƒm cá»±c tiá»ƒu. Náº¿u learning rate giáº£m quÃ¡ cháº­m, hÃ m chi phÃ­ cÃ³ thá»ƒ thay Ä‘á»•i lÃªn xuá»‘ng má»©c tá»‘i thiá»ƒu trong má»™t thá»i gian dÃ i vÃ  káº¿t thÃºc báº±ng má»™t nghiá»‡m tá»‘i Æ°u náº¿u báº¡n dá»«ng viá»‡c training quÃ¡ sá»›m.</p>

<p>Äoáº¡n code sau mÃ´ phá»ng viá»‡c giáº£m dáº§n learning rate trong quÃ¡ trÃ¬nh training:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td> --><td class="rouge-code"><pre><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">t0</span><span class="p">,</span> <span class="n">t1</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span> <span class="c1"># learning schedule hyperparameters
</span><span class="k">def</span> <span class="nf">learning_schedule</span><span class="p">(</span><span class="n">t</span><span class="p">):</span> <span class="n">returnt0</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="n">t1</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># random initialization
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
		<span class="n">random_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
		<span class="n">xi</span> <span class="o">=</span> <span class="n">X_b</span><span class="p">[</span><span class="n">random_index</span><span class="p">:</span><span class="n">random_index</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">random_index</span><span class="p">:</span><span class="n">random_index</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="n">gradients</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span><span class="n">xi</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">yi</span><span class="p">)</span> <span class="n">eta</span> <span class="o">=</span> <span class="n">learning_schedule</span><span class="p">(</span><span class="n">epoch</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> 			<span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradients</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="Ä‘Ã¡nh-giÃ¡-sgd">ÄÃ¡nh giÃ¡ SGD</h2>

<p>Thuáº­t toÃ¡n láº·p Ä‘i láº·p láº¡i bá»Ÿi cÃ¡c vÃ²ng láº·p m, má»—i láº§n láº·p láº¡i Ä‘Æ°á»£c gá»i lÃ  1 epoc. Trong bÃ i trÆ°á»›c khi sá»­ dá»¥ng Batch GD láº·p láº¡i 1000 láº§n thÃ¬ SGD chá»‰ cáº§n Ä‘áº¿n 50 láº§n lÃ  Ä‘Æ°á»£c káº¿t quáº£ khÃ¡ tá»‘t.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td> --><td class="rouge-code"><pre>&gt;&gt;&gt; theta
array([[4.21076011],
[2.74856079]])
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="lÆ°u-Ã½-vá»›i-sgd">LÆ°u Ã½ vá»›i SGD</h2>

<p>VÃ¬ viá»‡c lá»±a chá»n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n lÃ  ngáº«u nhiÃªn nÃªn 1 vÃ i Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c chá»n nhiá»u láº§n á»Ÿ cÃ¡c epoc khÃ¡c nhau vÃ  1 sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u thÃ¬ láº¡i khÃ´ng Ä‘Æ°á»£c dÃ¹ng Ä‘áº¿n. Cho nÃªn náº¿u báº¡n muá»‘n cháº¯c cháº¯n thuáº­t toÃ¡n Ä‘i qua háº¿t cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thÃ¬ cÃ¡ch tá»‘t nháº¥t lÃ  sáº¯p xáº¿p láº¡i ngáº«u nhiÃªn táº­p huáº¥n luyá»‡n sau má»—i epoc.</p>

<p>Äá»ƒ thá»±c hiá»‡n Linear Regression sá»­ dá»¥ng SGD vá»›i Scikit-Learn, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng SGDRegressor class. Máº·c Ä‘á»‹nh nÃ³ sáº½ tá»‘i Æ°u bÃ¬nh phÆ°Æ¡ng hÃ m chi phÃ­ , Ä‘oáº¡n code sau sáº½ cháº¡y tá»‘i Ä‘a 1000 epochs (max_iter=1000)  hoáº·c Ä‘áº¿n khi hÃ m máº¥t mÃ¡t nhá» hÆ¡n 1e-3 (tol=1e-3). Báº¯t Ä‘áº§u vá»›i learning rate = 0.1 (eta0 = 0.01). Sá»­ dá»¥ng hÃ m lÃªn lá»‹ch máº·c Ä‘á»‹nh:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td> --><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="n">sgd_reg</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="n">sgd_reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="c1">#&gt;&gt;&gt; sgd_reg.intercept_, sgd_reg.coef_
#(array([4.24365286]),array([2.8250878]))
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h1>

<p>Thuáº­t toÃ¡n Gradient Descent cuá»‘i cÃ¹ng mÃ  chÃºng ta nghiÃªn cá»©u Ä‘Ã³ lÃ  Mini-batch Gradient Descent. Náº¿u chÃºng ta hiá»ƒu vá» batch GD vÃ  SGD thÃ¬ sáº½ dá»… dÃ ng hiá»ƒu vá» Mini-batch GD: á»Ÿ má»—i bÆ°á»›c, thay vÃ¬ tÃ­nh toÃ¡n vector Ä‘á»™ dá»‘c dá»±a trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n (nhÆ° tháº±ng Batch GD) hoáº·c dá»±a trÃªn 1 Ä‘iá»ƒm dá»¯ liá»‡u (nhÆ° tháº±ng Stochastic GD) thÃ¬ Mini-batch GD tÃ­nh gradients dá»±a trÃªn 1 táº­p nhá» ngáº«u nhiÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c gá»i lÃ  mini- batches. Æ¯u Ä‘iá»ƒm chÃ­nh cá»§a Mini-batch GD so vá»›i Stochastic GD Ä‘Ã³ lÃ  chÃºng ta cÃ³ thá»ƒ táº­n dá»¥ng tá»‘i Ä‘a Ä‘Æ°á»£c hiá»‡u suáº¥t pháº§n cá»©ng khi thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n trÃªn ma tráº­n, vÃ­ dá»¥ nhÆ° GPU (táº­n dá»¥ng kháº£ nÄƒng tÃ­nh toÃ¡n song song cá»§a pháº§n cá»©ng)</p>

<blockquote>
  <p>Mini-batch GD sáº½ tiáº¿n Ä‘áº¿n gáº§n cá»±c tiá»ƒu toÃ n cá»¥c hÆ¡n SGD nhÆ°ng sáº½ khÃ³ Ä‘á»ƒ thoÃ¡t khá»i cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng hÆ¡n. DÃ¹ váº­y SGD vÃ  Mini-batch GD sáº½ tiáº¿n Ä‘áº¿n cá»±c tiá»ƒu toÃ n cá»¥c náº¿u chÃºng ta Ã¡p dá»¥ng giáº£m dáº§n learning rate há»£p lÃ½.</p>
</blockquote>

<p><img src="/assets/img/blog/image-20.jpg" alt="Mini-batch Gradient Descent" />
<em>Mini-batch Gradient Descent</em></p>

:ET