I"÷-<p>NhÆ° chÃºng ta Ä‘Ã£ tháº£o luáº­n tá»« cÃ¡c bÃ i trÆ°á»›c, má»™t vÃ i thuáº­t toÃ¡n há»“i quy cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n lá»›p (vÃ  ngÆ°á»£c láº¡i) Ä‘á»u cho káº¿t quáº£ khÃ¡ tá»‘t. Logistic Regression (hay cÃ²n gá»i lÃ  Logit Regression) Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n Ä‘á»ƒ Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t 1 Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ thá»ƒ thuá»™c vá» 1 lá»›p nÃ o Ä‘Ã³ (vÃ­ dá»¥ tÃ­nh xÃ¡c suáº¥t Ä‘á»ƒ 1 email lÃ  spam). Náº¿u xÃ¡c suáº¥t &gt; 50% thÃ¬ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cÃ³ thá»ƒ kháº³ng Ä‘á»‹nh Ä‘Æ°á»£c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã³ thuá»™c vá» lá»›p 1 (nhÃ£n lÃ  1) hoáº·c ngÆ°á»£c láº¡i (nhÃ£n lÃ  0). Viá»‡c nÃ y Ä‘Æ°á»£c gá»i lÃ  phÃ¢n loáº¡i nhá»‹ phÃ¢n (chá»‰ cÃ³ 0 vá»›i 1)</p>

<h1 id="Æ°á»›c-lÆ°á»£ng-xÃ¡c-suáº¥t">Æ¯á»›c lÆ°á»£ng xÃ¡c suáº¥t</h1>

<p>LÃ m tháº¿ nÃ o Ä‘á»ƒ Æ°á»›c lÆ°á»£ng Ä‘Æ°á»£c xÃ¡c suáº¥t 1 Ä‘iá»ƒm dá»¯ liá»‡u thuá»™c vÃ o lá»›p nÃ o? giá»‘ng nhÆ° mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh, mÃ´ hÃ¬nh há»“i quy Logistic sáº½ tÃ­nh tá»•ng cÃ¡c tÃ­ch trá»ng sá»‘ (weighted sum) vá»›i features Ä‘áº§u vÃ o (cÃ³ cá»™ng thÃªm bias). NhÆ°ng thay vÃ¬ tráº£ káº¿t quáº£ trá»±c tiáº¿p giá»‘ng mÃ´ hÃ¬nh Linear Regression thÃ¬ nÃ³ sáº½ Ä‘i qua 1 hÃ m logistic Ä‘á»ƒ cho káº¿t quáº£.</p>

\[\hat p = h_Î¸(x) = Ïƒ(x^TÎ¸)  (\text{HÃ m Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t})\]

<p>Trong Ä‘Ã³ Ïƒ(Â·) lÃ  hÃ m sigmoid, Ä‘áº§u ra cá»§a nÃ³ sáº½ lÃ  khoáº£ng giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 1.</p>

\[\sigma(t) = \frac{1}{1 + exp(-t)}\]

<p>ChÃºng ta nháº­n xÃ©t Ä‘Ã¢y lÃ  má»™t hÃ m sá»‘ Ä‘áº·c biá»‡t, vá»›i má»i giÃ¡ trá»‹ cá»§a t, hÃ m luÃ´n náº±m trong khoáº£ng [0,1] vÃ  cÃ³ Ä‘á»“ thá»‹ nhÆ° sau:</p>

<p><img src="/assets/img/blog/1_a04iKNbchayCAJ7-0QlesA.png" alt="Äá»“ thá»‹ hÃ m sigmoid" />
<em>Äá»“ thá»‹ hÃ m sigmoid</em></p>

<p>Sau khi Ä‘Ã£ tÃ­nh Ä‘Æ°á»£c xÃ¡c suáº¥t $\hat p = h_\theta(x)$ ta sáº½ dá»… dÃ ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c x thuá»™c vá» lá»›p nÃ o, hay  $\hat y$:</p>

\[\hat y =\begin{cases}0 &amp; \text{if $\hat p  &lt; 0.5 $} \\1 &amp; \text{if $\hat p \ge 0.5 $}\end{cases}\]

<p>Äá»ƒ Ã½ ráº±ng $Ïƒ(t) &lt; 0.5$ khi $t&lt;0$ vÃ  $Ïƒ(t) \ge 0.5$ khi $t \ge 0$ vÃ¬ váº­y há»“i quy Logistic sáº½ dá»± Ä‘oÃ¡n lÃ  1 khi $x^T \theta$ lÃ  dÆ°Æ¡ng vÃ  0 khi $x^T \theta$ Ã¢m.</p>

<h1 id="hÃ m-chi-phÃ­-vÃ -huáº¥n-luyá»‡n-mÃ´-hÃ¬nh">HÃ m chi phÃ­ vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh</h1>

<p>VÃ¬ ta phÃ¢n lá»›p dá»±a trÃªn xÃ¡c suáº¥t nÃªn hÃ m chi phÃ­ sáº½ Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn xÃ¡c suáº¥t xáº£y ra. VÃ¬ váº­y má»¥c tiÃªu cá»§a viá»‡c training sáº½ lÃ  tÃ¬m ra vector tham sá»‘ Î¸ Ä‘á»ƒ mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t cao cho cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thuá»™c vÃ o lá»›p 1 (positive) vÃ  xÃ¡c suáº¥t tháº¥p cho cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thuá»™c vÃ o lá»›p 2 (negative) nÃªn ta sáº½ cÃ³ hÃ m chi phÃ­ cho Ä‘iá»ƒm dá»¯ liá»‡u x nhÆ° sau:</p>

\[c(\theta) =\begin{cases}-log(\hat p) &amp; \text{if $ y = 1 $} \\-log(1- \hat p) &amp; \text{if $ y = 0 $}\end{cases}\]

<p>HÃ m chi phÃ­ ráº¥t â€œnháº¡yâ€ bá»Ÿi vÃ¬ â€“ log(t) sáº½ tÄƒng nhanh khi t xáº¥p xá»‰ 0. Náº¿u ta dá»± Ä‘oÃ¡n dá»¯ liá»‡u thuá»™c lá»›p 1 cÃ³ xÃ¡c xuáº¥t nhá» thÃ¬ chi phÃ­ sáº½ lá»›n (trÆ°á»ng há»£p dá»± Ä‘oÃ¡n sai) vÃ  náº¿u ta dá»¯ Ä‘oÃ¡n dá»¯ liá»‡u thuá»™c lá»›p 0 mÃ  cÃ³ xÃ¡c suáº¥t nhá» thÃ¬ chi phÃ­ sáº½ nhá» vÃ  xáº¥p xá»‰ dáº§n vá» 0. ÄÃ¢y chÃ­nh lÃ  má»¥c tiÃªu cá»§a chÃºng ta khi huáº¥n luyá»‡n mÃ´ hÃ¬nh.</p>

<p>HÃ m chi phÃ­ trong toÃ n bá»™ táº­p huáº¥n chá»‰ Ä‘Æ¡n giáº£n lÃ  chi phÃ­ trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n. NÃªn chÃºng ta cÃ³ thá»ƒ viáº¿t gá»™p láº¡i thÃ nh biá»ƒu thá»©c sau:</p>

\[J(\theta)=-\frac{1}{m}\sum_{i=1}{m}[y^{(i)} log(\hat p^{(i)}) + (1-y^{(i)})log(1-\hat p^{(i)})]\]

<p>á» cÃ´ng thá»©c nÃ y chÃºng ta khÃ´ng thá»ƒ tÃ­nh Ä‘Æ°á»£c tham sá»‘ Î¸ Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a hÃ m chi phÃ­ má»™t cÃ¡ch trá»±c tiáº¿p. Tuy nhiÃªn thÃ¬ hÃ m nÃ y lÃ  hÃ m lá»“i, vÃ¬ váº­y nÃªn ta cÃ³ thá»ƒ sá»­ dá»¥ng Gradient Descent  (hoáº·c báº¥t kÃ¬ thuáº­t toÃ¡n tá»‘i Æ°u nÃ o) nhÆ° á»Ÿ bÃ i trÆ°á»›c Ä‘á»ƒ thá»±c hiá»‡n tÃ¬m cá»±c tiá»ƒu (náº¿u learning rate khÃ´ng quÃ¡ lá»›n vÃ  báº¡n chá» Ä‘á»§ lÃ¢u). Äáº¡o hÃ m tá»«ng pháº§n cá»§a hÃ m chi phÃ­ $ \theta_j $ Ä‘Æ°á»£c tÃ­nh nhÆ° sau (tiáº¿n hÃ nh Ä‘áº¡o hÃ m theo Î¸):</p>

\[\frac{\delta J(\theta) }{\delta \theta_j}= \frac{1}{m}\sum_{1}^{m}(\sigma (\theta^Tx^{(i)}) - y^{(i)})x_j^{(i)}\]

<p>Qua má»—i Ä‘iá»ƒm dá»¯ liá»‡u, chÃºng ta sáº½ tÃ­nh toÃ¡n lá»—i cá»§a viá»‡c dá»± Ä‘oÃ¡n vÃ  nhÃ¢n nÃ³ vá»›i x thá»© j rá»“i thá»±c hiá»‡n tÃ­nh trung bÃ¬nh trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n.</p>

<p>Khi Ä‘Ã³ chÃºng ta cÃ³  gradient vector chá»©a táº¥t cáº£ cÃ¡c Ä‘áº¡o hÃ m riÃªng Ä‘á»ƒ thá»±c hiá»‡n thuáº­t toÃ¡n Batch Gradient Descent. VÃ  theo cÃ´ng thá»©c cáº­p nháº­t Î¸  chÃºng ta sáº½ cÃ³:</p>

<p>$ \theta = \theta - Î·\frac{\delta J(\theta)}{\delta \theta} $ Vá»›i Î· lÃ  learning rate</p>

<p>VÃ  thay vÃ¬ tÃ­nh toÃ¡n trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n, báº¡n cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng Mini-batch GD hoáº·c Stochastic GD nhÆ° mÃ¬nh giá»›i thiá»‡u á»Ÿ bÃ i trÆ°á»›c.</p>

<h1 id="láº­p-trÃ¬nh">Láº­p trÃ¬nh</h1>
<p>MÃ¬nh sáº½ thá»­ láº­p trÃ¬nh vá»›i BGD, táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y lÃ  dá»¯ liá»‡u bÃ i táº­p trong khoÃ¡ há»c ML cá»§a giÃ¡o sÆ° Andrew Ng.</p>

<p>MÃ£ nguá»“n cÃ¡c báº¡n cÃ³ thá»ƒ xem táº¡i Ä‘Ã¢y, dataset thÃ¬ táº¡i Ä‘Ã¢y (https://github.com/dinhquy94/codecamp.vn/blob/master/ex2.ipynb)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td class="rouge-code"><pre><span class="c1"># gradient descent max step
</span><span class="n">INTERATIONS</span> <span class="o">=</span> <span class="mi">200000</span>
<span class="c1"># learning rate
</span><span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># calc sigmoid function
</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="c1"># calc J function
</span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="c1"># number of training examples
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span>
    <span class="c1"># activation
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span>
    <span class="c1"># cost
</span>    <span class="n">j</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span>  <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">j</span>

<span class="c1"># implement BGD
</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_inters</span><span class="p">):</span>
    <span class="c1"># number of training examples
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span>
    <span class="n">jHistory</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_inters</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_inters</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span><span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>
        <span class="n">jHistory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">jHistory</span>

<span class="c1"># train
</span><span class="n">theta</span><span class="p">,</span> <span class="n">jHistory</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">ALPHA</span><span class="p">,</span> <span class="n">INTERATIONS</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="c1"># theta: [-7.45017822  0.06550395  0.05898701]
</span></pre></td></tr></tbody></table></code></pre></div></div>
<p>Káº¿t quáº£</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre># theta: [-7.45017822  0.06550395  0.05898701]
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Nguá»“n tham kháº£o: Blog Do Minh Hai</p>

:ET