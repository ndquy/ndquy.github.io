I"Îæ<p>N·∫øu ƒë√£ t√¨m hi·ªÉu v·ªÅ machine learning, ch·∫Øc c√°c b·∫°n ƒë∆∞·ª£c nghe r·∫•t nhi·ªÅu ƒë·∫øn kh√°i ni·ªám h√†m m·∫•t m√°t.</p>

<p>Trong c√°c thu·∫≠t to√°n t√¨m ki·∫øm c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o c·ªï ƒëi·ªÉn, h√†m m·∫•t m√°t c√≥ th·ªÉ l√† m·ªôt h√†m m·ª•c ti√™u c·ªßa qu√° tr√¨nh t√¨m ki·∫øm. Qu√° tr√¨nh t√¨m ki·∫øm s·∫Ω th·ª±c hi·ªán c√°c thay ƒë·ªïi hay ph∆∞∆°ng ph√°p di chuy·ªÉn ƒë·ªÉ h√†m m·ª•c ti√™u c√≥ gi√° tr·ªã nh·ªè nh·∫•t ho·∫∑c gi√° tr·ªã ch·∫•p nh·∫≠n ƒë∆∞·ª£c.</p>

<p>C√≤n trong lƒ©nh v·ª±c h·ªçc m√°y, b·∫£n ch·∫•t c·ªßa qu√° tr√¨nh h·ªçc m√°y l√† v·ªõi m·ªói d·ªØ li·ªáu ƒë·∫ßu v√†o trong qu√° tr√¨nh hu·∫•n luy·ªán, thu·∫≠t to√°n s·∫Ω t√¨m c√°ch thay ƒë·ªïi c√°c tham s·ªë b√™n trong m√¥ h√¨nh ƒë·ªÉ m√¥ h√¨nh tr·ªü n√™n t·ªët h∆°n trong vi·ªác ‚Äúd·ª± ƒëo√°n‚Äù ·ªü t∆∞∆°ng lai v·ªõi nh·ªØng d·ªØ li·ªáu ƒë·∫ßu v√†o x·∫•p x·ªâ ho·∫∑c t∆∞∆°ng t·ª±. Vi·ªác thay ƒë·ªïi tr·ªçng s·ªë c·ªßa m√¥ h√¨nh th∆∞·ªùng ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°c thu·∫≠t to√°n di chuy·ªÉn theo ƒë·ªô d·ªëc (hay c√≤n g·ªçi l√† Gradient descend).</p>

<p>H√†m m·∫•t m√°t ·ªü ƒë√¢y s·∫Ω ƒë√≥ng vai tr√≤ ƒë√°nh gi√° ƒë·ªô ‚Äút·ªët‚Äù c·ªßa m√¥ h√¨nh v·ªõi m·ªôt b·ªô tr·ªçng s·ªë t∆∞∆°ng ·ª©ng. M·ª•c ƒë√≠ch c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán l√† t√¨m ra b·ªô s·ªë ƒë·ªÉ ƒë·ªô l·ªõn h√†m m·∫•t m√°t (loss function) l√† nh·ªè nh·∫•t (c·ª±c ti·ªÉu). Nh∆∞ v·∫≠y ta c√≥ th·ªÉ coi h√†m m·∫•t m√°t l√† h√†m m·ª•c ti√™u trong qu√° tr√¨nh hu·∫•n luy·ªán.</p>

<p>L√† m·ªôt ph·∫ßn c·ªßa thu·∫≠t to√°n t·ªëi ∆∞u h√≥a, loss ƒë·ªëi v·ªõi tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa m√¥ h√¨nh ph·∫£i ƒë∆∞·ª£c ∆∞·ªõc l∆∞·ª£ng l·∫∑p l·∫°i. ƒêi·ªÅu n√†y ƒë√≤i h·ªèi ph·∫£i l·ª±a ch·ªçn m·ªôt h√†m m·ª•c ti√™u, c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ∆∞·ªõc t√≠nh ƒë·ªô l·ªói c·ªßa m√¥ h√¨nh ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tr·ªçng s·ªë nh·∫±m gi·∫£m l·ªói trong l·∫ßn ƒë√°nh gi√° ti·∫øp theo.</p>

<h1 id="m·ª•c-ti√™u">M·ª•c ti√™u</h1>
<p>Trong th·ª±c t·∫ø vi·ªác l·ª±a ch·ªçn h√†m m·∫•t m√°t ·∫£nh h∆∞·ªüng r·∫•t nhi·ªÅu ƒë·∫øn ch·∫•t l∆∞·ª£ng c·ªßa m√¥ h√¨nh khi hu·∫•n luy·ªán. B√†i vi·∫øt n√†y s·∫Ω cung c·∫•p cho c√°c b·∫°n n·ªôi dung v·ªÅ c√°c h√†m m·∫•t m√°t hay s·ª≠ d·ª•ng, so s√°nh v√† ƒë√°nh gi√° c√°c h√†m m·∫•t m√°t trong m·ªôt s·ªë b√†i to√°n c·ª• th·ªÉ.</p>

<p>M√¥ h√¨nh m·∫°ng n∆°ron h·ªçc c√°ch √°nh x·∫° t·ª´ inputs v√†o output t·ª´ c√°c examples v√† l·ª±a ch·ªçn h√†m m·∫•t m√°t ph·∫£i ph√π h·ª£p v·ªõi t·ª´ng m√¥ h√¨nh d·ª± ƒëo√°n c·ª• th·ªÉ, v√≠ d·ª• nh∆∞ c√°c b√†i to√°n nh∆∞ ph√¢n lo·∫°i ho·∫∑c h·ªìi quy.</p>

<p>Trong h∆∞·ªõng d·∫´n n√†y, b·∫°n s·∫Ω kh√°m ph√° c√°ch ch·ªçn m·ªôt h√†m m·∫•t m√°t cho m·∫°ng n∆°-ron h·ªçc s√¢u c·ªßa m√¨nh cho m·ªôt v·∫•n ƒë·ªÅ m√¥ h√¨nh d·ª± ƒëo√°n nh·∫•t ƒë·ªãnh. N·ªôi dung b√†i vi·∫øt g·ªìm:</p>

<ul>
  <li>C√°ch ƒë·ªÉ thi·∫øt l·∫≠p model cho mean squared error v√† c√°c bi·∫øn th·ªÉ c·ªßa h·ªìi quy (regression)</li>
  <li>C√°ch thi·∫øt l·∫≠p model cho cross-entropy v√† h√†m m·∫•t m√°t cho b√†i to√°n binary classification.</li>
  <li>C√°ch thi·∫øt l·∫≠p model cho cross-entropy v√† KL divergence loss functions cho b√†i to√°n multi-class classification.</li>
</ul>

<h1 id="regression-loss-functions">Regression Loss Functions</h1>

<p>M·ªôt b√†i to√°n s·ª≠ d·ª•ng m√¥ h√¨nh d·ª± b√°o h·ªìi quy th∆∞·ªùng li√™n quan ƒë·∫øn vi·ªác d·ª± ƒëo√°n m·ªôt ƒë·∫°i l∆∞·ª£ng c√≥ gi√° tr·ªã th·ª±c. V√≠ d·ª• b√†i to√°n d·ª± ƒëo√°n gi√° nh√†, d·ª± ƒëo√°n gi√° c·ªï phi·∫øu‚Ä¶</p>

<p>Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω kh·∫£o s√°t c√°c loss function ph√π h·ª£p cho c√°c b√†i to√°n regression.</p>

<p>ƒê·ªÉ t·∫°o d·ªØ li·ªáu demo cho b√†i to√°n regression, m√¨nh s·∫Ω s·ª≠ d·ª•ng h√†m make_regression() c√≥ s·∫µn trong th∆∞ vi·ªán c·ªßa scikit-learn. H√†m n√†y s·∫Ω t·∫°o d·ªØ li·ªáu m·∫´u v·ªõi c√°c bi·∫øn ƒë·∫ßu v√†o, nhi·ªÖu v√† c√°c thu·ªôc t√≠nh kh√°c‚Ä¶</p>

<p>Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m n√†y ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu g·ªìm 20 features, 10 features c√≥ √Ω nghƒ©a v·ªÅ m·∫∑t d·ªØ li·ªáu v√† 10 features kh√¥ng c√≥ √Ω nghƒ©a. M√¨nh s·∫Ω t·∫°o 1,000 ƒëi·ªÉm d·ªØ li·ªáu ng·∫´u nhi√™n cho b√†i to√°n. Tham s·ªë random_state s·∫Ω ƒë·∫£m b·∫£o cho ch√∫ng ta c√°c d·ªØ li·ªáu l√† nh∆∞ nhau m·ªói l·∫ßn ch·∫°y.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># generate regression dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>M·∫°ng n∆° ron nh√¨n chung s·∫Ω ho·∫°t ƒë·ªông t·ªët h∆°n khi c√°c gi√° tr·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o v√† ƒë·∫ßu ra c·ªßa m√¥ h√¨nh ƒë∆∞·ª£c scale v·ªÅ c√πng m·ªôt mi·ªÅn gi√° tr·ªã. Khi ƒë√≥ thu·∫≠t to√°n h·ªçc s·∫Ω h·ªôi t·ª• nhanh h∆°n vi·ªác thu√¥c t√≠nh d·ªØ li·ªáu kh√¥ng c√πng mi·ªÅn gi√° tr·ªã. Trong b√†i to√°n n√†y, bi·∫øn target ·ªü d·∫°ng ph√¢n ph·ªëi Gaussion, do v·∫≠y vi·ªác chu·∫©n h√≥a d·ªØ li·ªáu r·∫•t c·∫ßn thi·∫øt.</p>

<p>M√¨nh s·∫Ω s·ª≠ d·ª•ng class StandardScaler trong th∆∞ vi·ªán scikit-learn. C√≤n trong th·ª±c t·∫ø ch√∫ng ta n√™n x√¢y d·ª±ng m·ªôt b·ªô scaler cho c·∫£ t·∫≠p training v√† t·∫≠p testing. ƒê·ªÉ ƒë∆°n gi·∫£n, m√¨nh s·∫Ω scale d·ªØ li·ªáu tr∆∞·ªõc khi chia ra l√†m t·∫≠p train v√† t·∫≠p test</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="c1"># chu·∫©n h√≥a d·ªØ li·ªáu
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="mi">1</span><span class="p">))[:,</span><span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi scale xong, ch√∫ng ta s·∫Ω chia th√†nh t·∫≠p train v√† t·∫≠p test:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>ƒê·ªÉ demo vi·ªác t√¨m hi·ªÉu v·ªÅ h√†m m·∫•t m√°t, m√¨nh s·∫Ω s·ª≠ d·ª•ng m·ªôt model ƒë∆°n gi·∫£n ƒë√≥ l√† Multilayer Perceptron (MLP).</p>

<p>Model s·∫Ω g·ªìm ƒë·∫ßu v√†o l√† 20 features, m√¥ h√¨nh s·∫Ω c√≥ 1 l·ªõp ·∫©n v·ªõi 25 nodes, sau ƒë√≥ s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t ReLU. ƒê·∫ßu ra s·∫Ω g·ªìm 1 node t∆∞∆°ng ·ª©ng v·ªõi gi√° tr·ªã ƒë·∫ßu ra mu·ªën d·ª± ƒëo√°n, cu·ªëi c√πng s·∫Ω l√† m·ªôt h√†m k√≠ch ho·∫°t tuy·∫øn t√≠nh .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>M√¨nh s·∫Ω fit m√¥ h√¨nh n√†y v·ªõi thu·∫≠t to√°n t·ªëi ∆∞u stochastic gradient descent v√† s·ª≠ d·ª•ng learning rate l√† 0.01, momentum  0.9</p>

<p>Vi·ªác hu·∫•n luy·ªán s·∫Ω th·ª±c hi·ªán qua 100 epochs v√† s·ª≠ d·ª•ng t·∫≠p testing ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh sau m·ªói epoch. Cu·ªëi c√πng ta c√≥ th·ªÉ v·∫Ω l·∫°i ƒë∆∞·ª£c learning curves sau khi th·ª±c hi·ªán xong.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'...'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="c1"># fit model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>V·∫≠y l√† b√¢y gi·ªù ch√∫ng ta ƒë√£ c√≥ b√†i to√°n v√† m√¥ h√¨nh, ti·∫øp theo m√¨nh s·∫Ω ƒë√°nh gi√° 3 h√†m m·∫•t m√°t ph·ªï bi·∫øn th√≠ch h·ª£p cho c√°c b√†i to√°n h·ªìi quy.</p>

<h2 id="mean-squared-error-loss">Mean Squared Error Loss</h2>

<p>Mean Square Error (MSE) hay c√≤n ƒë∆∞·ª£c g·ªçi l√† L2 Loss l√† m·ªôt loss function c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c m√¥ h√¨nh h·ªìi quy, ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh. MSE ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng c√°c b√¨nh ph∆∞∆°ng c·ªßa hi·ªáu gi·ªØa gi√° tr·ªã th·ª±c (y : target) v√† gi√° tr·ªã m√† m√¥ h√¨nh c·ªßa ch√∫ng ra d·ª± ƒëo√°n (y^:predicted).</p>

<p>MSE c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong keras v·ªõi gi√° tr·ªã l√† ‚Äòmse‚Äò ho·∫∑c ‚Äòmean_squared_error‚Äò khi hu·∫•n luy·ªán m√¥ h√¨nh</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>MSE ƒë∆∞·ª£c ∆∞u ti√™n d√πng cho m√¥ h√¨nh c√≥ layer ƒë·∫ßu ra c√≥ 1 node v√† s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t tuy·∫øn t√≠nh</p>

<p>V√≠ d·ª•:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code ho√†n ch·ªânh s·∫Ω nh∆∞ sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for regression with mse loss function
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># T·∫°o dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Chu·∫©n h√≥a d·ªØ li·ªáu
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="mi">1</span><span class="p">))[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Chia t·∫≠p d·ªØ li·ªáu training v√† testing
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># ƒê·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="c1"># Hu·∫•n luy·ªán m√¥ h√¨nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ƒê√°nh gi√° m√¥ h√¨nh
</span><span class="n">train_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">test_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_mse</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">))</span>
<span class="c1"># V·∫Ω l·∫°i ƒë·ªì th·ªã h√†m m·∫•t m√°t trong qu√° tr√¨nh hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss / Mean Squared Error'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi ch·∫°y, k·∫øt qu·∫£ s·∫Ω in ra gi√° tr·ªã MSE tr√™n t·∫≠p train v√† t·∫≠p test</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<p>K·∫øt qu·∫£ in ra s·∫Ω l√†:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Train: 0.000, Test: 0.001
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán gi√° tr·ªã MSE trong qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa t·∫≠p train (m√†u xanh) v√† t·∫≠p test (m√†u cam)</p>

<p><img src="/assets/img/blog/Line-plot-of-Mean-Squared-Error-Loss-over-Training-Epochs-When-Optimizing-the-Mean-Squared-Error-Loss-Function.webp" alt="K·∫øt qu·∫£ hu·∫•n luy·ªán" />
<em>K·∫øt qu·∫£ hu·∫•n luy·ªán d·ª±a tr√™n loss</em></p>

<p>Ch√∫ng ta c√≥ th·ªÉ th·∫•y m√¥ h√¨nh ƒë√£ h·ªôi t·ª• nhanh ch√≥ng. Nh∆∞ v·∫≠y MSE th·ªÉ hi·ªán t·ªët trong tr∆∞·ªùng h·ª£p n√†y.</p>

<h2 id="mean-squared-logarithmic-error-loss">Mean Squared Logarithmic Error Loss</h2>

<p>C√≥ m·ªôt v·∫•n ƒë·ªÅ v·ªõi c√°c m√¥ h√¨nh h·ªìi quy, ƒë√≥ l√† gi√° tr·ªã d·ª± ƒëo√°n c√≥ s·ª± ch√™nh l·ªách l·ªõn ho·∫∑c r·∫•t l·ªõn, nghƒ©a l√† khi ch√∫ng ta d·ª± ƒëo√°n ƒë∆∞·ª£c m·ªôt gi√° tr·ªã l·ªõn, ta kh√¥ng c·∫ßn ph·∫£i ƒë√°nh ph·∫°t tr·ªçng s·ªë m·ªôt c√°ch n·∫∑ng n·ªÅ (nghƒ©a l√† c√°c tr·ªçng s·ªë kh√¥ng n√™n ƒë∆∞·ª£c thay ƒë·ªïi nhi·ªÅu) nh∆∞ khi d√πng MSE.</p>

<p>Thay v√†o ƒë√≥, tr∆∞·ªõc ti√™n b·∫°n c√≥ th·ªÉ l·∫•y logarit c·ªßa t·ª´ng gi√° tr·ªã d·ª± ƒëo√°n, sau ƒë√≥ t√≠nh sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh. ƒê√¢y ƒë∆∞·ª£c g·ªçi l√† m·∫•t m√°t l·ªói l√¥garit trung b√¨nh b√¨nh ph∆∞∆°ng, vi·∫øt t·∫Øt l√† MSLE.</p>

<p>N√≥ c√≥ √Ω nghƒ©a l√† gi·∫£m vi·ªác ph·∫°t tr·ªçng s·ªë khi d·ª± ƒëo√°n ƒë∆∞·ª£c m·ªôt gi√° tr·ªã l·ªõn.</p>

<p>Nh∆∞ m·ªôt ph√©p ƒëo s·ª± m·∫•t m√°t, ƒëi·ªÅu n√†y s·∫Ω gi√∫p m√¥ h√¨nh x·∫•p x·ªâ t·ªët h∆°n khi d·ª± ƒëo√°n c√°c gi√° tr·ªã ch∆∞a ƒë∆∞·ª£c scale. M√¨nh s·∫Ω ch·ª©ng minh h√†m m·∫•t m√°t n√†y b·∫±ng m·ªôt b√†i to√°n regression ƒë∆°n gi·∫£n:</p>

<p>M√¨nh s·∫Ω thay ƒë·ªïi h√†m m·∫•t m√°t khi hu·∫•n luy·ªán b·∫±ng h√†m ‚Äòmean_squared_logarithmic_error‚Äò v√† ƒë·ªÉ nguy√™n c√°c m√¥ h√¨nh ·ªü c√°c layer ƒë·∫ßu ra. Sau ƒë√≥ m√¨nh s·∫Ω t√≠nh sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh ƒë·ªÉ v·∫Ω ƒë·ªì th·ªã.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_logarithmic_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mse'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code ho√†n ch·ªânh nh∆∞ sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for regression with mse loss function
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># T·∫°o dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Chu·∫©n h√≥a d·ªØ li·ªáu
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="mi">1</span><span class="p">))[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Chia t·∫≠p d·ªØ li·ªáu training v√† testing
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># ƒê·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_logarithmic_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mse'</span><span class="p">])</span>
<span class="c1"># Hu·∫•n luy·ªán m√¥ h√¨nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ƒê√°nh gi√° m√¥ h√¨nh
</span><span class="n">train_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">test_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_mse</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">))</span>
<span class="c1"># V·∫Ω l·∫°i ƒë·ªì th·ªã h√†m m·∫•t m√°t trong qu√° tr√¨nh hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Mean Squared Error'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'mean_squared_error'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_mean_squared_error'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ch·∫°y ƒëo·∫°n code s·∫Ω s·∫Ω in ra l·ªói b√¨nh ph∆∞∆°ng trung b√¨nh cho m√¥ h√¨nh khi hu·∫•n luy·ªán v√† t·∫≠p d·ªØ li·ªáu th·ª≠ nghi·ªám.</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<p>Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng m√¥ h√¨nh d·∫´n ƒë·∫øn MSE k√©m h∆°n m·ªôt ch√∫t tr√™n c·∫£ t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† ki·ªÉm tra. N√≥ c√≥ th·ªÉ kh√¥ng ph√π h·ª£p cho b√†i to√°n n√†y v√¨ ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u l√† m·ªôt ph√¢n ph·ªëi chu·∫©n Gaussian .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">Train</span><span class="p">:</span> <span class="mf">0.165</span><span class="p">,</span> <span class="n">Test</span><span class="p">:</span> <span class="mf">0.184</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Bi·ªÉu ƒë·ªì sau th·ªÉ hi·ªán c√°c gi√° tr·ªã loss MSLE qua m·ªói epochs, ƒë∆∞·ªùng m√†u xanh th·ªÉ hi·ªán tr√™n t·∫≠p hu·∫•n luy·ªán, m√†u cam th·ªÉ hi·ªán tr√™n t·∫≠p test</p>

<p>Ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng MSLE ƒë√£ h·ªôi t·ª• t·ªët khi th·ª±c hi·ªán ƒë∆∞·ª£c 100 epochs; c√≤n MSE c√≥ v·∫ª nh∆∞ thay ƒë·ªïi qu√° m·ª©c, loss gi·∫£m nhanh v√† b·∫Øt ƒë·∫ßu tƒÉng t·ª´ 20 epochs tr·ªü ƒëi.</p>

<p><img src="/assets/img/blog/Line-plots-of-Mean-Squared-Logistic-Error-Loss-and-Mean-Squared-Error-over-Training-Epochs.webp" alt="Line Plots of Mean Squared Logarithmic Error Loss and Mean Squared Error Over Training Epochs" />
<em>Line Plots of Mean Squared Logarithmic Error Loss and Mean Squared Error Over Training Epochs</em></p>

<h2 id="mean-absolute-error-loss">Mean Absolute Error Loss</h2>

<p>Trong m·ªôt s·ªë b√†i to√°n h·ªìi quy, ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u c√≥ th·ªÉ ch·ªß y·∫øu l√† ph√¢n ph·ªëi Gaussian, nh∆∞ng c√≥ th·ªÉ c√≥ c√°c gi√° tr·ªã ngo·∫°i l·ªá, v√≠ d·ª•: gi√° tr·ªã l·ªõn ho·∫∑c nh·ªè kh√°c xa v·ªõi gi√° tr·ªã trung b√¨nh.</p>

<p>Mean Absolute Error (MAE) hay c√≤n ƒë∆∞·ª£c g·ªçi l√† L1 Loss l√† m·ªôt loss function ƒë∆∞·ª£c s·ª≠ d·ª•ng cho
c√°c m√¥ h√¨nh h·ªìi quy, ƒë·∫∑c bi·ªát cho c√°c m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh. MAE ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng c√°c
tr·ªã tuy·ªát ƒë·ªëi c·ªßa hi·ªáu gi·ªØa gi√° tr·ªã th·ª±c (y : target) v√† gi√° tr·ªã m√† m√¥ h√¨nh c·ªßa ch√∫ng ra d·ª± ƒëo√°n (y^: predicted).</p>

<p><img src="/assets/img/blog/mae.png" alt="C√¥ng th·ª©c" /></p>

<p>M√¨nh s·∫Ω thay ƒë·ªïi h√†m m·∫•t m√°t khi hu·∫•n luy·ªán b·∫±ng h√†m ‚Äòmean_absolute_error‚Äò v√† ƒë·ªÉ nguy√™n c√°c m√¥ h√¨nh ·ªü c√°c layer ƒë·∫ßu ra. Sau ƒë√≥ m√¨nh s·∫Ω t√≠nh sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh ƒë·ªÉ v·∫Ω ƒë·ªì th·ªã.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_absolute_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mse'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code ho√†n ch·ªânh nh∆∞ sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for regression with mae loss function
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># generate regression dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># standardize dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="mi">1</span><span class="p">))[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_absolute_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mse'</span><span class="p">])</span>
<span class="c1"># fit model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># evaluate the model
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_mse</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_mse</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">))</span>
<span class="c1"># plot loss during training
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># plot mse during training
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Mean Squared Error'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'mean_squared_error'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_mean_squared_error'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code s·∫Ω in ra gi√° tr·ªã MLSE cho m√¥ h√¨nh tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p th·ª≠ nghi·ªám.</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Train: 0.002, Test: 0.002
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng MAE th·ª±c s·ª± h·ªôi t·ª• nh∆∞ng v·∫´n c√≥ ƒë∆∞·ªùng g·∫≠p gh·ªÅnh, m·∫∑c d√π t·ªïng qu√°t c·ªßa MSE kh√¥ng b·ªã ·∫£nh h∆∞·ªüng nhi·ªÅu. Ch√∫ng ta bi·∫øt r·∫±ng ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u l√† m·ªôt ph√¢n ph·ªëi Gaussian chu·∫©n kh√¥ng c√≥ gi√° tr·ªã ngo·∫°i l·ªá l·ªõn, v√¨ v·∫≠y MAE s·∫Ω kh√¥ng ph√π h·ª£p trong tr∆∞·ªùng h·ª£p n√†y.</p>

<p><img src="/assets/img/blog/Line-plots-of-Mean-Absolute-Error-Loss-and-Mean-Squared-Error-over-Training-Epochs.webp" alt="Line plots of Mean Absolute Error Loss and Mean Squared Error over Training Epochs" /></p>

<h1 id="t·ªïng-k·∫øt">T·ªïng k·∫øt</h1>

<p>Trong ph·∫ßn 1 m√¨nh ƒë√£ gi·ªõi thi·ªáu cho c√°c b·∫°n 2 h√†m loss ƒë∆∞·ª£c d√πng cho b√†i to√°n regression, trong b√†i ti·∫øp theo (p2) m√¨nh s·∫Ω gi·ªõi thi·ªáu h√†m loss cho b√†i to√°n Binary classification v√† ph·∫ßn 3 l√† c√°c h√†m loss cho b√†i to√°n ph√¢n ƒëa l·ªõp.</p>

:ET