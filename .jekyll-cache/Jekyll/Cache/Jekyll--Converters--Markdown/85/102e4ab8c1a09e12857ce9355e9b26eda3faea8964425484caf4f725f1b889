I"¢£<p>PhÃ¢n Ä‘a lá»›p lÃ  nhá»¯ng bÃ i toÃ¡n mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n trong Ä‘Ã³ cÃ¡c Ä‘áº§u vÃ o Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh lÃ  má»™t trong nhiá»u hÆ¡n hai lá»›p.</p>

<p>MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ sá»‘ nguyÃªn, trong Ä‘Ã³ má»—i lá»›p Ä‘Æ°á»£c gÃ¡n má»™t giÃ¡ trá»‹ sá»‘ nguyÃªn duy nháº¥t tá»« â€‹â€‹0 Ä‘áº¿n (num_classes - 1). BÃ i toÃ¡n thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° lÃ  dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a Ä‘iá»ƒm dá»¯ liá»‡u thuá»™c vá» má»™t lá»›p nÃ o Ä‘Ã³ Ä‘Ã£ biáº¿t. num_classes lÃ  sá»‘ class</p>

<p>Trong pháº§n nÃ y, chÃºng ta sáº½ kháº£o sÃ¡t cÃ¡c hÃ m máº¥t mÃ¡t thÃ­ch há»£p cho cÃ¡c mÃ´ hÃ¬nh phÃ¢n Ä‘a lá»›p.</p>

<h1 id="khá»Ÿi-táº¡o-dá»¯-liá»‡u">Khá»Ÿi táº¡o dá»¯ liá»‡u</h1>

<p>MÃ¬nh sáº½ sá»­ dá»¥ng bÃ i toÃ¡n blobs (cÃ¡c Ä‘á»‘m mÃ u) lÃ m cÆ¡ sá»Ÿ Ä‘á»ƒ thá»­ nghiá»‡m cÃ¡c hÃ m máº¥t mÃ¡t. 
HÃ m make_blobs () Ä‘Æ°á»£c cung cáº¥p bá»Ÿi thÆ° viá»‡n scikit-learning cung cáº¥p Ä‘á»ƒ táº¡o cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  cÃ¡c lá»›p tÆ°Æ¡ng á»©ng vá»›i Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã³.
MÃ¬nh sáº½ sá»­ dá»¥ng hÃ m nÃ y Ä‘á»ƒ táº¡o ra 1.000 Ä‘iá»ƒm dá»¯ liá»‡u cho bÃ i toÃ¡n phÃ¢n loáº¡i 3 lá»›p vá»›i 2 biáº¿n Ä‘áº§u vÃ o. 
TrÃ¬nh táº¡o sá»‘ giáº£ ngáº«u nhiÃªn sáº½ Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ 1.000 vÃ­ dá»¥ giá»‘ng nhau Ä‘Æ°á»£c táº¡o ra má»—i khi cháº¡y. (random_state)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># generate dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Hai biáº¿n Ä‘áº§u vÃ o lÃ  x vÃ  y tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘iá»ƒm trong khÃ´ng gian 2 chiá»u.</p>

<p>Code dÆ°á»›i Ä‘Ã¢y táº¡o ra má»™t biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n sá»± phÃ¢n tÃ¡n cá»§a toÃ n bá»™ táº­p dá»¯ liá»‡u, cÃ¡c Ä‘iá»ƒm Ä‘Æ°á»£c tÃ´ mÃ u theo lá»›p tÆ°Æ¡ng á»©ng.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c1"># scatter plot of blobs dataset
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">where</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># generate dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># select indices of points with each class label
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
	<span class="n">samples_ix</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
	<span class="n">pyplot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">samples_ix</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">samples_ix</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/img/blog/Scatter-Plot-of-Examples-Generated-from-the-Blobs-Multi-Class-Classification-Problem.webp" alt="Dá»¯ liá»‡u cho bÃ i toÃ¡n phÃ¢n Ä‘a lá»›p" />
<em>Dá»¯ liá»‡u cho bÃ i toÃ¡n phÃ¢n Ä‘a lá»›p</em></p>

<p>CÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u sáº½ khÃ´ng rescale chÃºng trong trÆ°á»ng há»£p nÃ y.</p>

<h2 id="phÃ¢n-chia-dá»¯-liá»‡u-train-test">PhÃ¢n chia dá»¯ liá»‡u train-test</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># phÃ¢n chia dá»¯ liá»‡u train-test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="chuáº©n-bá»‹-mÃ´-hÃ¬nh">Chuáº©n bá»‹ mÃ´ hÃ¬nh</h2>

<p>ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh MLP Ä‘Æ¡n giáº£n Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n nÃ y. BÃ i toÃ¡n sáº½ gá»“m Ä‘áº§u vÃ o vá»›i 2 features, má»™t lá»›p áº©n vá»›i 50 nÃºt. HÃ m kÃ­ch hoáº¡t tuyáº¿n tÃ­nh vÃ  layer Ä‘áº§u ra mÃ¬nh sáº½ lá»±a chá»n theo má»—i hÃ m máº¥t mÃ¡t sáº½ sá»­ dá»¥ng.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(...,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'...'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>MÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c fit báº±ng thuáº­t toÃ¡n SGD vá»›i learning rate lÃ  0.01 vÃ  momentum lÃ  0.9</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'...'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>MÃ¬nh sáº½ huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i 100 epochs sau Ä‘Ã³ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i loss vÃ  Ä‘á»™ chÃ­nh xÃ¡c (accuracy) á»Ÿ má»—i epoch. Sau Ä‘Ã³ mÃ¬nh sáº½ váº½ learning curves.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># fit model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi Ä‘Ã£ Ä‘á»‹nh nghÄ©a xong mÃ´ hÃ¬nh, bÃ¢y giá» mÃ¬nh sáº½ tiáº¿n hÃ nh thá»­ nghiá»‡m cÃ¡c hÃ m lá»—i khÃ¡c nhau vÃ  so sÃ¡nh káº¿t quáº£ giá»¯a cÃ¡c hÃ m loss Ä‘á»ƒ Ä‘Æ°a ra nháº­n xÃ©t cho má»—i phÆ°Æ¡ng phÃ¡p.</p>

<h1 id="multi-class-cross-entropy-loss">Multi-Class Cross-Entropy Loss</h1>

<p>Cross-entropy Ä‘Æ°á»£c sá»­ dá»¥ng máº·c Ä‘á»‹nh cho cÃ¡c bÃ i toÃ¡n phÃ¢n Ä‘a lá»›p. Trong bÃ i toÃ¡n phÃ¢n Ä‘a lá»›p, má»¥c Ä‘Ã­ch cá»§a mÃ´ hÃ¬nh lÃ  dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a má»™t Ä‘iá»ƒm dá»¯ liá»‡u rÆ¡i vÃ o class (lá»›p) nÃ o trong sá»‘ cÃ¡c class {0, 1, 3, â€¦, n}, má»—i class tÆ°Æ¡ng á»©ng vá»›i má»™t sá»‘ nguyÃªn</p>

<p>Vá» máº·t toÃ¡n há»c, hÃ m cross-entropy loss Ä‘Æ°á»£c Æ°u tiÃªn sá»­ dá»¥ng. ÄÃ¢y lÃ  hÃ m Ä‘Ã¡nh giÃ¡ sá»­ dá»¥ng Ä‘áº§u tiÃªn, vÃ  ta chá»‰ thay Ä‘á»•i hÃ m nÃ y náº¿u cÃ³ lÃ½ do nÃ o khÃ¡c Ä‘áº·c biá»‡t.
y Ä‘á»•i náº¿u báº¡n cÃ³ lÃ½ do chÃ­nh Ä‘Ã¡ng.</p>

<p>Nháº¯c láº¡i bÃ i trÆ°á»›c, cross-entropy tÃ­nh khoáº£ng cÃ¡ch giá»¯a 2 phÃ¢n bá»‘ xÃ¡c suáº¥t.</p>

\[H(\mathbf{p}, \mathbf{q}) = \mathbf{E_p}[-\log \mathbf{q}]\]

<p>Vá»›i p vÃ  q lÃ  rá»i ráº¡c (nhÆ° y - nhÃ£n tháº­t sá»± vÃ  y^ - nhÃ£n dá»± Ä‘oÃ¡n ) trong bÃ i toÃ¡n cá»§a chÃºng ta), cÃ´ng thá»©c nÃ y Ä‘Æ°á»£c viáº¿t dÆ°á»›i dáº¡ng:</p>

\[H(\mathbf{p}, \mathbf{q}) =-\sum_{i=1}^C p_i \log q_i ~~~ (1)\]

<p>Trong Ä‘Ã³ C lÃ  sá»‘ lÆ°á»£ng cÃ¡c class cáº§n phÃ¢n lá»›p, trong bÃ i toÃ¡n binary classification thÃ¬ C = 2.</p>

<p>Cross-entropy Ä‘Æ°á»£c cung cáº¥p trong Keras báº±ng cÃ¡ch thiáº¿t láº­p tham sá»‘ loss=â€˜categorical_crossentropyâ€˜ khi compile mÃ´ hÃ¬nh.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>HÃ m yÃªu cáº§u lá»›p Ä‘áº§u ra Ä‘Æ°á»£c thiáº¿t láº­p vá»›i n node (má»™t nÃºt cho má»—i class), trong trÆ°á»ng há»£p nÃ y lÃ  3 node vÃ  mÃ¬nh sá»­ dá»¥ng hÃ m kÃ­ch hoáº¡t â€˜softmaxâ€˜ Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cho má»—i lá»›p.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>NhÆ° váº­y biáº¿n Ä‘Ã­ch y pháº£i dÆ°á»›i dáº¡ng one-hot encoding. Trong Ä‘Ã³ vá»‹ trÃ­ cá»§a sá»‘ 1 trong vector sau khi biáº¿n Ä‘á»•i tÆ°Æ¡ng á»©ng vá»›i class cá»§a input. Ta thá»±c hiá»‡n Ä‘Æ°a Ä‘áº§u ra y thÃ nh dáº¡ng one-hot báº±ng hÃ m Keras.to_categorical()</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># one hot encode output variable
</span><span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code Ä‘áº§y Ä‘á»§ sá»­ dá»¥ng hÃ m loss Cross-entropy Ä‘Æ°á»£c trÃ¬nh bÃ y dÆ°á»›i Ä‘Ã¢y:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for the blobs multi-class classification problem with cross-entropy loss
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># Khá»Ÿi táº¡o dá»¯ liá»‡u 2 chiá»u cho bÃ i toÃ¡n phÃ¢n Ä‘a lá»›p
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># biáº¿n Ä‘á»•i Ä‘áº§u ra thÃ nh dáº¡ng one-hot encoding
</span><span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># Ä‘á»‹nh nghÄ©a mÃ´ hÃ¬nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="c1"># compile mÃ´ hÃ¬nh
</span><span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># huáº¥n luyá»‡n mÃ´ hÃ¬nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
<span class="c1"># váº½ Ä‘á»“ thá»‹ thá»ƒ hiá»‡n giÃ¡ trá»‹ loss trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># váº½ Ä‘á»“ thá»‹ thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi cháº¡y, káº¿t quáº£ sáº½ in ra Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p train vÃ  táº­p test</p>

<blockquote>
  <p>ChÃº Ã½ khi cháº¡y, káº¿t quáº£ cÃ³ thá»ƒ khÃ¡c nhau do thuáº­t toÃ¡n khá»Ÿi táº¡o ngáº«u nhiÃªn. ChÃºng ta nÃªn cháº¡y nhiá»u láº§n vÃ  láº¥y giÃ¡ trá»‹ trung bÃ¬nh</p>
</blockquote>

<p>Káº¿t quáº£ in ra sáº½ lÃ :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Train: 0.840, Test: 0.822
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong trÆ°á»ng há»£p nÃ y chÃºng ta cÃ³ thá»ƒ tháº¥y mÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c khÃ¡ tá»‘t, 84% trÃªn táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  82% trÃªn táº­p test.</p>

<p>Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng thá»ƒ hiá»‡n giÃ¡ trá»‹ cross-entropy trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a táº­p train (mÃ u xanh) vÃ  táº­p test (mÃ u cam)</p>

<p><img src="/assets/img/blog/Line-Plots-of-Cross-Entropy-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Blobs-Multi-Class-Classification-Problem.webp" alt="Äá»“ thá»‹ Ä‘Æ°á»ng cá»§a hÃ m máº¥t mÃ¡t Cross Entropy vÃ  Ä‘á»™ chÃ­nh xÃ¡c" />
<em>Äá»“ thá»‹ Ä‘Æ°á»ng cá»§a hÃ m máº¥t mÃ¡t Cross Entropy vÃ  Ä‘á»™ chÃ­nh xÃ¡c</em></p>

<p>Trong trÆ°á»ng há»£p nÃ y, biá»ƒu Ä‘á»“ cho tháº¥y mÃ´ hÃ¬nh cÃ³ váº» nhÆ° Ä‘Ã£ há»™i tá»¥. 
CÃ¡c Ä‘á»“ thá»‹ Ä‘Æ°á»ng cho cáº£ cross-entropy vÃ  Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»u cho tháº¥y sá»± há»™i tá»¥ tá»‘t, máº·c dÃ¹ hÆ¡i nháº¥p nhÃ´. 
MÃ´ hÃ¬nh cÃ³ váº» tá»‘t, khÃ´ng bá»‹ underfit hay overfit. 
CÃ²n Ä‘á»ƒ biá»ƒu Ä‘á»“ mÆ°á»£t hÆ¡n, chÃºng ta cÃ³ thá»ƒ Ä‘iá»u chá»‰nh batch size hoáº·c learning rate.</p>

<h1 id="sparse-multiclass-cross-entropy-loss">Sparse Multiclass Cross-Entropy Loss</h1>

<p>Khi sá»­ dá»¥ng one-hot encoding Ä‘á»ƒ Ä‘Æ°a cÃ¡c nhÃ£n vá» dáº¡ng vector sáº½ xáº£y ra má»™t sá»‘ váº¥n Ä‘á» khi cÃ³ nhiá»u nhÃ£n cáº§n phÃ¢n loáº¡i.
VÃ­ dá»¥, dá»± Ä‘oÃ¡n cÃ¡c tá»« trong má»™t bá»™ ngá»¯ liá»‡u cÃ³ thá»ƒ cÃ³ hÃ ng chá»¥c hoáº·c hÃ ng trÄƒm nghÃ¬n tá»« vá»±ng khÃ¡c nhau, má»—i loáº¡i tÆ°Æ¡ng á»©ng vá»›i má»™t nhÃ£n. 
Äiá»u nÃ y cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ Ä‘Ã­ch y cá»§a má»—i Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ thá»ƒ lÃ  má»™t one-hot vectÆ¡ vá»›i hÃ ng chá»¥c hoáº·c hÃ ng trÄƒm nghÃ¬n sá»‘ 0, lÃºc nÃ y sáº½ khÃ¡ tá»‘n bá»™ nhá»›.</p>

<p>Sparse cross-entropy giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch thá»±c hiá»‡n cÃ¹ng má»™t phÃ©p tÃ­nh toÃ¡n Ä‘á»™ lá»—i cá»§a cross-entropy mÃ  khÃ´ng yÃªu cáº§u biáº¿n Ä‘Ã­ch y pháº£i Ä‘Æ°á»£c Ä‘Æ°a vá» dáº¡ng one-hot trÆ°á»›c khi huáº¥n luyá»‡n.
Cross-entropy Ä‘Æ°á»£c cung cáº¥p trong Keras báº±ng cÃ¡ch thiáº¿t láº­p tham sá»‘ loss=â€˜sparse_categorical_crossentropyâ€˜ khi compile mÃ´ hÃ¬nh.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>HÃ m yÃªu cáº§u layer Ä‘áº§u ra Ä‘Æ°á»£c gá»“m n node tÆ°Æ¡ng á»©ng vá»›i sá»‘ class vÃ  sá»­ dá»¥ng kÃ­ch hoáº¡t â€˜softmaxâ€™ Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cho Ä‘áº§u ra cho má»—i lá»›p.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Lá»£i Ã­ch khi sá»­ dá»¥ng hÃ m máº¥t mÃ¡t sparse cross-entropy Ä‘Ã³ lÃ  khÃ´ng cáº§n pháº£i thá»±c hiá»‡n one-hot encoding.</p>

<p>Code Ä‘áº§y Ä‘á»§ sá»­ dá»¥ng hÃ m loss sparse cross-entropy Ä‘Æ°á»£c trÃ¬nh bÃ y dÆ°á»›i Ä‘Ã¢y:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for the blobs multi-class classification problem with sparse cross-entropy loss
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># generate 2d classification dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="c1"># compile model
</span><span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># fit model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># evaluate the model
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
<span class="c1"># plot loss during training
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># plot accuracy during training
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi cháº¡y, káº¿t quáº£ sáº½ in ra Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p train vÃ  táº­p test</p>

<blockquote>
  <p>ChÃº Ã½ khi cháº¡y, káº¿t quáº£ cÃ³ thá»ƒ khÃ¡c nhau do thuáº­t toÃ¡n khá»Ÿi táº¡o ngáº«u nhiÃªn. ChÃºng ta nÃªn cháº¡y nhiá»u láº§n vÃ  láº¥y giÃ¡ trá»‹ trung bÃ¬nh</p>
</blockquote>

<p>Trong trÆ°á»ng há»£p nÃ y chÃºng ta cÃ³ thá»ƒ tháº¥y mÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c 83% trÃªn táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  81% trÃªn táº­p test. Káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p test vÃ  táº­p train khÃ¡ gáº§n nhau chá»©ng tá» mÃ´ hÃ¬nh khÃ´ng bá»‹ underfit hay overfit.
TrÃªn thá»±c táº¿, náº¿u báº¡n láº·p láº¡i thá»­ nghiá»‡m nhiá»u láº§n, Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh cá»§a cross-entropy vÃ  sparse cross-entropy sáº½ cÃ³ thá»ƒ so sÃ¡nh Ä‘Æ°á»£c.</p>

<p>Káº¿t quáº£ in ra sáº½ lÃ :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Train: 0.832, Test: 0.818
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vÃ  giÃ¡ trá»‹ máº¥t mÃ¡t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a táº­p train (mÃ u xanh) vÃ  táº­p test (mÃ u cam)</p>

<p><img src="/assets/img/blog/Line-Plots-of-Sparse-Cross-Entropy-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Blobs-Multi-Class-Classification-Problem.webp" alt="Sparse Cross-entropy vÃ  Classification Accuracy" />
<em>Sparse Cross-entropy vÃ  Classification Accuracy</em></p>

<h1 id="tá»•ng-káº¿t">Tá»•ng káº¿t</h1>

<p>NhÆ° váº­y mÃ¬nh Ä‘Ã£ káº¿t thÃºc 3 pháº§n bÃ n vá» hÃ m loss, qua 3 pháº§n mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u cho cÃ¡c báº¡n cÃ¡c hÃ m loss hay sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n hay gáº·p.
NgoÃ i ra cÃ²n nhiá»u hÃ m loss khÃ¡c mÃ¬nh chÆ°a cÃ³ Ä‘iá»u kiá»‡n Ä‘á»ƒ giá»›i thiá»‡u á»Ÿ Ä‘Ã¢y, háº¹n gáº·p má»i ngÆ°á»i á»Ÿ cÃ¡c bÃ i viáº¿t sau.</p>

<h1 id="tham-kháº£o">Tham kháº£o</h1>

<h2 id="posts">Posts</h2>
<ul>
  <li><a href="https://machinelearningcoban.com/2017/04/13/softmarginsmv/">Soft Margin Support Vector Machine</a>.</li>
  <li><a href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/">Loss and Loss Functions for Training Deep Learning Neural Networks</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li><a href="https://arxiv.org/abs/1702.05659">On Loss Functions for Deep Neural Networks in Classification</a>, 2017.</li>
</ul>

<h2 id="api">API</h2>

<ul>
  <li><a href="https://keras.io/losses/">Keras Loss Functions API</a></li>
  <li><a href="https://keras.io/activations/">Keras Activation Functions API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">sklearn.preprocessing.StandardScaler API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html">sklearn.datasets.make_regression API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">sklearn.datasets.make_circles API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">sklearn.datasets.make_blobs API</a></li>
</ul>

<h2 id="articles">Articles</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean squared error, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean absolute error, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Hinge_loss">Hinge loss, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullbackâ€“Leibler divergence, Wikipedia</a>.</li>
  <li><a href="https://isaacchanghau.github.io/post/loss_functions/">Loss Functions in Neural Networks</a>, 2017.</li>
</ul>

:ET