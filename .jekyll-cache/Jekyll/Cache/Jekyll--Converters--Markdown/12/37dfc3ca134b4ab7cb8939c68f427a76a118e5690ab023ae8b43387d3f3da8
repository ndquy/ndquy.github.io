I"≠”<p>Ph√¢n l·ªõp nh·ªã ph√¢n l√† b√†i to√°n m√† bi·∫øn ƒë·∫ßu ra (y) ch·ªâ nh·∫≠n m·ªôt trong hai gi√° tr·ªã l√† 1 trong 2 nh√£n.</p>

<p>B√†i to√°n th∆∞·ªùng d∆∞·ªõi d·∫°ng b√†i to√°n d·ª± ƒëo√°n gi√° tr·ªã 0 ho·∫∑c 1 cho l·ªõp ƒë·∫ßu ti√™n ho·∫∑c l·ªõp th·ª© hai v√† th∆∞·ªùng ƒë∆∞·ª£c ph√°t bi·ªÉu nh∆∞ d·ª± ƒëo√°n x√°c su·∫•t c·ªßa ƒë·∫ßu v√†o thu·ªôc gi√° tr·ªã l·ªõp 1.</p>

<p>Trong ph·∫ßn n√†y ch√∫ng ta s·∫Ω kh·∫£o s√°t c√°c h√†m loss cho b√†i to√°n ph√¢n l·ªõp nh·ªã ph√¢n.</p>

<h1 id="chu·∫©n-b·ªã-d·ªØ-li·ªáu-v√†-m√¥-h√¨nh">Chu·∫©n b·ªã d·ªØ li·ªáu v√† m√¥ h√¨nh</h1>

<p>M√¨nh s·∫Ω kh·ªüi t·∫°o d·ªØ li·ªáu th·ª≠ nghi·ªám b·∫±ng th∆∞ vi·ªán scikit-learn, d·ªØ li·ªáu c√°c l·ªõp s·∫Ω d∆∞·ªõi d·∫°ng h√¨nh tr√≤n. B√†i to√°n v·ªÅ c√°c v√≤ng tr√≤n li√™n quan ƒë·∫øn c√°c m·∫´u ƒë∆∞·ª£c v·∫Ω t·ª´ hai v√≤ng tr√≤n ƒë·ªìng t√¢m tr√™n m·ªôt m·∫∑t ph·∫≥ng hai chi·ªÅu, trong ƒë√≥ c√°c ƒëi·ªÉm tr√™n v√≤ng tr√≤n b√™n ngo√†i thu·ªôc l·ªõp 0 v√† c√°c ƒëi·ªÉm cho v√≤ng tr√≤n b√™n trong thu·ªôc l·ªõp 1. Nhi·ªÖu ƒë∆∞·ª£c th√™m v√†o c√°c m·∫´u ƒë·ªÉ tr√°nh overfit khi t√¨m hi·ªÉu.</p>

<p>M√¨nh s·∫Ω t·∫°o ra 1.000 ƒëi·ªÉm d·ªØ li·ªáu v√† th√™m 10% d·ªØ li·ªáu nhi·ªÖu . Tr√¨nh t·∫°o s·ªë gi·∫£ ng·∫´u nhi√™n s·∫Ω ƒë∆∞·ª£c thi·∫øt l·∫≠p v·ªõi c√πng 1 gi√° tr·ªã tham s·ªë ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng ch√∫ng ta lu√¥n nh·∫≠n ƒë∆∞·ª£c 1.000 ƒëi·ªÉm d·ªØ li·ªáu  gi·ªëng nhau ·ªü m·ªói l·∫ßn ch·∫°y kh√°c nhau.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># generate circles
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>M√¨nh s·∫Ω v·∫Ω th·ª≠ c√°c ƒëi·ªÉm d·ªØ li·ªáu:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># scatter plot of the circles dataset with points colored by class
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">where</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># generate circles
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># select indices of points with each class label
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
	<span class="n">samples_ix</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
	<span class="n">pyplot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">samples_ix</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">samples_ix</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p><img src="/assets/img/blog/Scatter-Plot-of-Dataset-for-the-Circles-Binary-Classification-Problem.webp" alt="Scatter Plot cho t·∫≠p d·ªØ li·ªáu Circles Binary Classification" />
<em>Scatter Plot cho t·∫≠p d·ªØ li·ªáu Circles Binary Classification</em></p>

<p>C√°c ƒëi·ªÉm d·ªØ li·ªáu n·∫±m xung quanh gi√° tr·ªã 0, g·∫ßn nh∆∞ trong kho·∫£ng [-1,1]. M√¨nh s·∫Ω kh√¥ng rescale ch√∫ng trong tr∆∞·ªùng h·ª£p n√†y.</p>

<p>Ph√¢n chia d·ªØ li·ªáu hu·∫•n luy·ªán, ki·ªÉm tra:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng m·ªôt m√¥ h√¨nh MLP ƒë∆°n gi·∫£n ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n n√†y. B√†i to√°n s·∫Ω g·ªìm ƒë·∫ßu v√†o v·ªõi 2 features, m·ªôt l·ªõp ·∫©n v·ªõi 50 n√∫t. H√†m k√≠ch ho·∫°t tuy·∫øn t√≠nh v√† layer ƒë·∫ßu ra m√¨nh s·∫Ω l·ª±a ch·ªçn theo m·ªói h√†m m·∫•t m√°t s·∫Ω s·ª≠ d·ª•ng.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># ƒê·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'...'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>M√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c fit b·∫±ng thu·∫≠t to√°n SGD v·ªõi learning rate l√† 0.01 v√† momentum l√† 0.9</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'...'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>M√¨nh s·∫Ω hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi 200 epochs sau ƒë√≥ ƒë√°nh gi√° m√¥ h√¨nh v·ªõi loss v√† ƒë·ªô ch√≠nh x√°c (accuracy) ·ªü m·ªói epoch. Sau ƒë√≥ m√¨nh s·∫Ω v·∫Ω learning curves.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># fit model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi ƒë√£ ƒë·ªãnh nghƒ©a xong m√¥ h√¨nh, b√¢y gi·ªù m√¨nh s·∫Ω ti·∫øn h√†nh th·ª≠ nghi·ªám c√°c h√†m l·ªói kh√°c nhau v√† so s√°nh k·∫øt qu·∫£ gi·ªØa c√°c h√†m loss ƒë·ªÉ ƒë∆∞a ra nh·∫≠n x√©t cho m·ªói ph∆∞∆°ng ph√°p.</p>

<h1 id="binary-cross-entropy-loss">Binary Cross-Entropy Loss</h1>

<p>Cross-entropy l√† h√†m loss ƒë∆∞·ª£c s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh cho b√†i to√°n ph√¢n l·ªõp nh·ªã ph√¢n.</p>

<p>N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n trong ƒë√≥ c√°c gi√° tr·ªã m·ª•c ti√™u nh·∫≠n m·ªôt trong 2 gi√° tr·ªã {0, 1}.</p>

<p>V·ªÅ m·∫∑t to√°n h·ªçc, n·∫øu nh∆∞ MSE t√≠nh kho·∫£ng c√°ch gi·ªØa 2 ƒë·∫°i l∆∞·ª£ng s·ªë th√¨ cross-entropy hi·ªÉu n√¥m na l√† ph∆∞∆°ng ph√°p t√≠nh kho·∫£ng c√°ch gi·ªØa 2 ph√¢n b·ªë x√°c su·∫•t.</p>

\[H(\mathbf{p}, \mathbf{q}) = \mathbf{E_p}[-\log \mathbf{q}]\]

<p>V·ªõi p v√† q l√† r·ªùi r·∫°c (nh∆∞ y - nh√£n th·∫≠t s·ª± v√† y^ - nh√£n d·ª± ƒëo√°n ) trong b√†i to√°n c·ªßa ch√∫ng ta), c√¥ng th·ª©c n√†y ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng:</p>

\[H(\mathbf{p}, \mathbf{q}) =-\sum_{i=1}^C p_i \log q_i ~~~ (1)\]

<p>Trong ƒë√≥ C l√† s·ªë l∆∞·ª£ng c√°c class c·∫ßn ph√¢n l·ªõp, trong b√†i to√°n binary classification th√¨ C = 2.</p>

<p>Cross-entropy ƒë∆∞·ª£c cung c·∫•p trong Keras b·∫±ng c√°ch thi·∫øt l·∫≠p tham s·ªë loss=‚Äòbinary_crossentropy‚Äò khi compile m√¥ h√¨nh.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>H√†m y√™u c·∫ßu layer ƒë·∫ßu ra ƒë∆∞·ª£c g·ªìm 1 node v√† s·ª≠ d·ª•ng k√≠ch ho·∫°t ‚Äòsigmoid‚Äô ƒë·ªÉ d·ª± ƒëo√°n x√°c su·∫•t cho ƒë·∫ßu ra.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Code ƒë·∫ßy ƒë·ªß s·ª≠ d·ª•ng h√†m loss Cross-entropy ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi ƒë√¢y:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for the circles problem with cross entropy loss
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="c1"># T·∫°o b·ªô d·ªØ li·ªáu 2 chi·ªÅu c·∫ßn ph√¢n l·ªõp 
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># ƒë·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># hu·∫•n luy·ªán m√¥ h√¨nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ƒê√°nh gi√° m√¥ h√¨nh
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
<span class="c1"># V·∫Ω ƒë·ªì th·ªã h√†m loss
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># V·∫Ω ƒë·ªì th·ªã ƒë·ªô ch√≠nh x√°c
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi ch·∫°y, k·∫øt qu·∫£ s·∫Ω in ra ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p train v√† t·∫≠p test</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<p>K·∫øt qu·∫£ in ra s·∫Ω l√†:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Train: 0.836, Test: 0.852
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong tr∆∞·ªùng h·ª£p n√†y ch√∫ng ta c√≥ th·ªÉ th·∫•y m√¥ h√¨nh ƒë·∫°t ƒë·ªô ch√≠nh x√°c 83% tr√™n t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† 85% tr√™n t·∫≠p test. K·∫øt qu·∫£ ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p test v√† t·∫≠p train kh√° g·∫ßn nhau ch·ª©ng t·ªè m√¥ h√¨nh kh√¥ng b·ªã underfit hay overfit.</p>

<p>Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán gi√° tr·ªã cross-entropy trong qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa t·∫≠p train (m√†u xanh) v√† t·∫≠p test (m√†u cam)</p>

<p><img src="/assets/img/blog/Line-Plots-of-Cross-Entropy-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Two-Circles-Binary-Classification-Problem.webp" alt="ƒê·ªì th·ªã ƒë∆∞·ªùng c·ªßa h√†m m·∫•t m√°t Cross Entropy v√† ƒë·ªô ch√≠nh x√°c" />
<em>ƒê·ªì th·ªã ƒë∆∞·ªùng c·ªßa h√†m m·∫•t m√°t Cross Entropy v√† ƒë·ªô ch√≠nh x√°c</em></p>

<p>Khi n√†o s·ª≠ d·ª•ng cross-entropy?</p>

<ul>
  <li>B√†i to√°n ph√¢n l·ªõp</li>
  <li>T·∫°o ra c√°c m√¥ h√¨nh v·ªõi ƒë·ªô ch·∫Øc ch·∫Øn cao (precision, recall cao)</li>
</ul>

<h1 id="hinge-loss">Hinge Loss</h1>

<p>M·ªôt c√°ch kh√°c ƒë·ªÉ t√≠nh cross-entropy cho b√†i to√°n ph√¢n l·ªõp nh·ªã ph√¢n ƒë√≥ l√† h√†m hinge loss. ƒê√¢y l√† √Ω t∆∞·ªüng ch√≠nh c·ªßa m√¥ h√¨nh Support Vector Machine (SVM).</p>

<p>N√≥ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n trong ƒë√≥ c√°c gi√° tr·ªã m·ª•c ti√™u nh·∫≠n m·ªôt trong 2 gi√° tr·ªã {-1, 1}.</p>

\[J_n(\mathbf{w}, b) = \max(0, 1 - y_nz_n)\]

<p>Trong ƒë√≥, $z_n$c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† score c·ªßa $x_n$ ·ª©ng v·ªõi c·∫∑p h·ªá s·ªë $(\mathbf{w}, b)$ ch√≠nh l√† ƒë·∫ßu ra mong mu·ªën.</p>

<p>H√†m loss s·∫Ω khuy·∫øn kh√≠ch c√°c ƒëi·ªÉm d·ªØ li·ªáu c√≥ d·∫•u ƒë√∫ng, ph·∫°t l·ªói n·∫∑ng h∆°n n·∫øu c√≥ gi√° tr·ªã d·ª± ƒëo√°n c√≥ d·∫•u kh√°c v·ªõi gi√° tr·ªã mong mu·ªën.</p>

<p>Trong th·ª±c t·∫ø, hinge loss th∆∞·ªùng th·ªÉ hi·ªán t·ªët h∆°n cross-entropy trong c√°c b√†i to√°n binary classification</p>

<p>ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω ƒë∆∞a c√°c nh√£n ƒë·∫ßu ra th√†nh m·ªôt trong 2 gi√° tr·ªã -1 v√† 1</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># ƒë·ªïi y t·ª´ {0,1} th√†nh {-1,1}
</span><span class="n">y</span><span class="p">[</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ch√∫ng ta c√≥ th·ªÉ ch·ªâ ƒë·ªãnh h√†m loss trong khi compile b·∫±ng gi√° tr·ªã loss=‚Äôhinge‚Äô</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'hinge'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Cu·ªëi c√πng, l·ªõp ƒë·∫ßu ra c·ªßa m√¥ h√¨nh s·∫Ω c√≥ 1 node v√† s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t activation=‚Äôtanh‚Äô ƒë·ªÉ ƒë·∫£m b·∫£o gi√° tr·ªã ƒë·∫ßu ra n·∫±m trong kho·∫£ng [-1, 1].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau ƒë√¢y l√† ƒëo·∫°n code ho√†n ch·ªânh:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for the circles problem with hinge loss
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">where</span>
<span class="c1"># T·∫°o d·ªØ li·ªáu 
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># ƒë∆∞a y t·ª´ {0,1} th√†nh {-1,1}
</span><span class="n">y</span><span class="p">[</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="c1"># Chia t·∫≠p d·ªØ li·ªáu th√†nh 2 t·∫≠p test v√† train
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># ƒë·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'hinge'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># hu·∫•n luy·ªán m√¥ h√¨nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ƒë√°nh gi√° m√¥ h√¨nh
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
<span class="c1"># v·∫Ω ƒë·ªì th·ªã gi√° tr·ªã loss sau khi hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># v·∫Ω ƒë·ªì th·ªã gi√° tr·ªã ƒë·ªô ch√≠nh x√°c sau khi hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi ch·∫°y, k·∫øt qu·∫£ s·∫Ω in ra ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh tr√™n t·∫≠p train v√† t·∫≠p test</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<p>K·∫øt qu·∫£ in ra s·∫Ω l√†:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">Train</span><span class="p">:</span> <span class="mf">0.792</span><span class="p">,</span> <span class="n">Test</span><span class="p">:</span> <span class="mf">0.740</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c√≥ th·ªÉ th·∫•y ƒë·ªô ch√≠nh x√°c k√©m h∆°n m·ªôt ch√∫t so v·ªõi vi·ªác s·ª≠ d·ª•ng cross-entropy, v·ªõi m√¥ h√¨nh ƒë√£ ch·ªçn c√≥ ƒë·ªô ch√≠nh x√°c d∆∞·ªõi 80% tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra.</p>

<p>Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán ƒë·ªô ch√≠nh x√°c trong qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa t·∫≠p train (m√†u xanh) v√† t·∫≠p test (m√†u cam)</p>

<p><img src="/assets/img/blog/Line-Plots-of-Hinge-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Two-Circles-Binary-Classification-Problem.webp" alt="Hinge Loss v√† Classification Accuracy" />
<em>Hinge Loss v√† Classification Accuracy</em></p>

<h1 id="squared-hinge-loss">Squared Hinge Loss</h1>

<p>H√†m hinge loss c√≥ nhi·ªÅu phi√™n b·∫£n m·ªü r·ªông, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m√¥ h√¨nh SVM.</p>

<p>M·ªôt phi√™n b·∫£n ph·ªï bi·∫øn c·ªßa hinge loss ƒë√≥ l√† squared hinge loss. N√≥ c√≥ t√°c d·ª•ng trong vi·ªác l√†m gi·∫£m s·ª± nh·∫•p nh√¥ c·ªßa ƒë·ªì th·ªã h√†m loss v√† d·ªÖ thao t√°c h∆°n v·ªÅ m·∫∑t to√°n h·ªçc.</p>

<p>N·∫øu h√†m hinge loss c√≥ k·∫øt qu·∫£ t·ªët trong b√†i to√°n ph√¢n l·ªõp nh·ªã ph√¢n th√¨ squared hinge loss c≈©ng s·∫Ω cho k·∫øt qu·∫£ t∆∞∆°ng t·ª±.</p>

<p>Gi·ªëng nh∆∞ hinge loss, ch√∫ng ta c≈©ng c·∫ßn bi·∫øn ƒë·ªïi c√°c nh√£n v·ªÅ gi√° tr·ªã -1 v√† 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># ƒë·ªïi y t·ª´ {0,1} th√†nh {-1,1}
</span><span class="n">y</span><span class="p">[</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ch√∫ng ta c√≥ th·ªÉ ch·ªâ ƒë·ªãnh h√†m loss trong khi compile b·∫±ng tham s·ªë</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'squared_hinge'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Cu·ªëi c√πng, l·ªõp ƒë·∫ßu ra c·ªßa m√¥ h√¨nh s·∫Ω c√≥ 1 node v√† s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t activation=‚Äôtanh‚Äô ƒë·ªÉ ƒë·∫£m b·∫£o gi√° tr·ªã ƒë·∫ßu ra n·∫±m trong kho·∫£ng [-1, 1].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau ƒë√¢y l√† ƒëo·∫°n code ho√†n ch·ªânh:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td class="rouge-code"><pre><span class="c1"># mlp for the circles problem with hinge loss
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">where</span>
<span class="c1"># T·∫°o d·ªØ li·ªáu 
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># ƒë∆∞a y t·ª´ {0,1} th√†nh {-1,1}
</span><span class="n">y</span><span class="p">[</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="c1"># Chia t·∫≠p d·ªØ li·ªáu th√†nh 2 t·∫≠p test v√† train
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">trainX</span><span class="p">,</span> <span class="n">testX</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">trainy</span><span class="p">,</span> <span class="n">testy</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="c1"># ƒë·ªãnh nghƒ©a m√¥ h√¨nh
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_uniform'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'squared_hinge'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># hu·∫•n luy·ªán m√¥ h√¨nh
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ƒë√°nh gi√° m√¥ h√¨nh
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testy</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train: %.3f, Test: %.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
<span class="c1"># v·∫Ω ƒë·ªì th·ªã gi√° tr·ªã loss sau khi hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># v·∫Ω ƒë·ªì th·ªã gi√° tr·ªã ƒë·ªô ch√≠nh x√°c sau khi hu·∫•n luy·ªán
</span><span class="n">pyplot</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">pyplot</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sau khi ch·∫°y, k·∫øt qu·∫£ s·∫Ω in ra ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh tr√™n t·∫≠p train v√† t·∫≠p test</p>

<blockquote>
  <p>Ch√∫ √Ω khi ch·∫°y, k·∫øt qu·∫£ c√≥ th·ªÉ kh√°c nhau do thu·∫≠t to√°n kh·ªüi t·∫°o ng·∫´u nhi√™n. Ch√∫ng ta n√™n ch·∫°y nhi·ªÅu l·∫ßn v√† l·∫•y gi√° tr·ªã trung b√¨nh</p>
</blockquote>

<p>K·∫øt qu·∫£ in ra s·∫Ω l√†:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">Train</span><span class="p">:</span> <span class="mf">0.682</span><span class="p">,</span> <span class="n">Test</span><span class="p">:</span> <span class="mf">0.646</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c√≥ th·ªÉ th·∫•y ƒë·ªô ch√≠nh x√°c k√©m h∆°n m·ªôt ch√∫t so v·ªõi vi·ªác s·ª≠ d·ª•ng Hinge loss, v·ªõi m√¥ h√¨nh ƒë√£ ch·ªçn c√≥ ƒë·ªô ch√≠nh x√°c d∆∞·ªõi 70% tr√™n t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra.</p>

<p>Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán ƒë·ªô ch√≠nh x√°c trong qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa t·∫≠p train (m√†u xanh) v√† t·∫≠p test (m√†u cam)</p>

<p><img src="/assets/img/blog/Line-Plots-of-Squared-Hinge-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Two-Circles-Binary-Classification-Problem.webp" alt="Squared Hinge Loss v√† Classification Accuracy" />
<em>Squared Hinge Loss v√† Classification Accuracy</em></p>

<p>M√¥ h√¨nh c√≥ v·∫ª ƒë√£ h·ªôi t·ª•, tuy nhi√™n b·ªÅ m·∫∑t h√†m l·ªói c√≤n nhi·ªÅu nh·∫•p nh√¥, ch·ª©ng t·ªè vi·ªác thay ƒë·ªïi tr·ªçng s·ªë nh·ªè ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn ƒë·ªô l·ªói c·ªßa m√¥ h√¨nh.</p>

<h1 id="t·ªïng-k·∫øt">T·ªïng k·∫øt</h1>

<p>Trong ph·∫ßn 2 m√¨nh ƒë√£ gi·ªõi thi·ªáu cho c√°c b·∫°n 2 h√†m loss ƒë∆∞·ª£c d√πng cho b√†i to√°n ph√¢n l·ªõp nh·ªã ph√¢n, trong b√†i ti·∫øp theo (p3) m√¨nh s·∫Ω gi·ªõi thi·ªáu h√†m loss cho b√†i to√°n Multi-Class Classification. (ph√¢n nhi·ªÅu l·ªõp)</p>

<h1 id="tham-kh·∫£o">Tham kh·∫£o</h1>

<h2 id="posts">Posts</h2>
<ul>
  <li><a href="https://machinelearningcoban.com/2017/04/13/softmarginsmv/">Soft Margin Support Vector Machine</a>.</li>
  <li><a href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/">Loss and Loss Functions for Training Deep Learning Neural Networks</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li><a href="https://arxiv.org/abs/1702.05659">On Loss Functions for Deep Neural Networks in Classification</a>, 2017.</li>
</ul>

<h2 id="api">API</h2>

<ul>
  <li><a href="https://keras.io/losses/">Keras Loss Functions API</a></li>
  <li><a href="https://keras.io/activations/">Keras Activation Functions API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">sklearn.preprocessing.StandardScaler API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html">sklearn.datasets.make_regression API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">sklearn.datasets.make_circles API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">sklearn.datasets.make_blobs API</a></li>
</ul>

<h2 id="articles">Articles</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean squared error, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean absolute error, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Hinge_loss">Hinge loss, Wikipedia</a>.</li>
  <li><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback‚ÄìLeibler divergence, Wikipedia</a>.</li>
  <li><a href="https://isaacchanghau.github.io/post/loss_functions/">Loss Functions in Neural Networks</a>, 2017.</li>
</ul>

:ET